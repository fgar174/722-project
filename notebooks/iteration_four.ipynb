{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/12 18:54:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F, window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "spark = SparkSession.builder.appName('Iteration4').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.optimizer.maxIterations\", 300)  \n",
    "\n",
    "\n",
    "\n",
    "import enum\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import pandas as pd #Used for typing process\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "databases_path = '../datasets/'\n",
    "PATH_IMAGES = '../tex/iterations/iteration_4/images/'\n",
    "PATH_MODELS = '../models/'\n",
    "CSV_GENERATED = \"../datasets/GENERATED/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Some methods and enums that will be used for:\n",
    "\n",
    "- Produce process images\n",
    "- Functions to execute repetitive tasks\n",
    "\n",
    "This will make easier the workflow of the project and also the artifact generation automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFramesCSV(enum.Enum):\n",
    "    ALCOHOL_CONSUMPTION_CSV = f\"{databases_path}4_total-alcohol-consumption-per-capita-litres-of-pure-alcohol.csv\"\n",
    "    COUNTRY_MASTER_CSV = f\"{databases_path}0_master_country_codes.csv\"\n",
    "    WHO_OBESITY_CSV = f\"{databases_path}1_who_obesity.csv\"\n",
    "    MEAT_CONSUMPTION_CSV = f\"{databases_path}2_meat_consumption.csv\"\n",
    "    HUNGER_CSV = f\"{databases_path}5_global_hunger_index.csv\"\n",
    "    SMOKING_CSV = f\"{databases_path}6_share-of-adults-who-smoke.csv\"\n",
    "    HAPPINESS_REPORT_CSV = f\"{databases_path}3_happiness_report.csv\"\n",
    "\n",
    "class DataFramePreviousFieldNameOptions(enum.Enum):\n",
    "    IS_NULL = 'isnull'\n",
    "    D_TYPES = 'dtypes'\n",
    "    COUNT = 'count'\n",
    "\n",
    "COLUMN_RENAME_BY_DATASET = {\n",
    "    DataFramesCSV.WHO_OBESITY_CSV: {\n",
    "        'Numeric': 'percentage_obesity',\n",
    "        'Countries, territories and areas': 'country',\n",
    "        'WHO region': 'region',\n",
    "        'Year': 'year',\n",
    "    },\n",
    "    DataFramesCSV.HAPPINESS_REPORT_CSV: {\n",
    "        'year': 'year',\n",
    "        'Country name': 'country',\n",
    "        \"Life Ladder\": 'life_ladder',\n",
    "        \"Social support\": 'social_support',\n",
    "        \"Freedom to make life choices\": \"freedom_to_make_life_choices\",\n",
    "        \"Generosity\": \"generosity\",\n",
    "        \"Perceptions of corruption\": \"perceptions_of_corruption\",\n",
    "        \"Positive affect\": \"positive_affect\",\n",
    "        \"Negative affect\": \"negative_affect\",\n",
    "    },\n",
    "    DataFramesCSV.MEAT_CONSUMPTION_CSV: {\n",
    "        'Code': 'country_code',\n",
    "        'Year': 'year',\n",
    "        \"Meat, poultry | 00002734 || Food available for consumption | 0645pc || kilograms per year per capita\": \"poultry\",\n",
    "        \"Meat, beef | 00002731 || Food available for consumption | 0645pc || kilograms per year per capita\": \"beef\",\n",
    "        \"Meat, sheep and goat | 00002732 || Food available for consumption | 0645pc || kilograms per year per capita\": \"sheep_and_goat\",\n",
    "        \"Meat, pig | 00002733 || Food available for consumption | 0645pc || kilograms per year per capita\": \"pig\",\n",
    "        \"Fish and seafood | 00002960 || Food available for consumption | 0645pc || kilograms per year per capita\": \"fish_and_seafood\",\n",
    "    },\n",
    "    DataFramesCSV.COUNTRY_MASTER_CSV: {\n",
    "        'alpha-3': 'country_code',\n",
    "        'name': 'country'\n",
    "    },\n",
    "    DataFramesCSV.HUNGER_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'Global Hunger Index (2021)': 'hunger_index',\n",
    "    },\n",
    "    DataFramesCSV.SMOKING_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'Prevalence of current tobacco use (% of adults)': 'prevalence_smoking',\n",
    "    },\n",
    "    DataFramesCSV.ALCOHOL_CONSUMPTION_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'liters_of_pure_alcohol_per_capita': 'liters_of_pure_alcohol_per_capita',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def capture_get_dataframe_info_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        previous_data: list = None,\n",
    "        previous_data_name: str = DataFramePreviousFieldNameOptions,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "):\n",
    "\n",
    "    # Collect required statistics from PySpark DataFrame\n",
    "    column_names = [col[:30] for col in df_spark.columns]\n",
    "    column_types = [dtype for _, dtype in df_spark.dtypes]\n",
    "    row_count = df_spark.count()\n",
    "    column_counts = [row_count for _ in df_spark.columns]\n",
    "    column_null_counts = df_spark.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_spark.columns]).collect()[0]\n",
    "    column_non_null_counts = [row_count - null_count for null_count in column_null_counts]\n",
    "\n",
    "    info_object = {\n",
    "        'columns': column_names,\n",
    "        DataFramePreviousFieldNameOptions.D_TYPES.value: column_types,\n",
    "        DataFramePreviousFieldNameOptions.COUNT.value: column_non_null_counts,\n",
    "        DataFramePreviousFieldNameOptions.IS_NULL.value: column_null_counts\n",
    "    }\n",
    "\n",
    "    if previous_data is not None:\n",
    "        info_object[f'old {previous_data_name.value}'] = previous_data\n",
    "        current_data = info_object[previous_data_name.value]\n",
    "        info_object['change'] = [prev - curr for prev, curr in zip(previous_data, current_data)]\n",
    "\n",
    "    dataframe_info = list(zip(*info_object.values()))\n",
    "\n",
    "    # Plotting the table image\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    col_widths = [0.35, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "    data_table = ax.table(\n",
    "        cellText=dataframe_info,\n",
    "        colLabels=[' '.join(col.split('_')) for col in info_object.keys()],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    if previous_data is not None:\n",
    "        for (i, j), val in np.ndenumerate(np.array(dataframe_info)):\n",
    "            if j == 5 and val != 0:\n",
    "                data_table.get_celld()[(i + 1), j].set_facecolor(\"red\")\n",
    "                data_table.get_celld()[(i + 1), j].set_text_props(color='white', weight='bold')\n",
    "\n",
    "    num_rows = row_count - 1\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(6)\n",
    "    plt.title(f'{table_name} (Records: {num_rows})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def capture_summary_dataset_to_image(\n",
    "        name_file: str,\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        dataset_name: str,\n",
    "        figure_size_height=3,\n",
    "        font_size=7,\n",
    "):\n",
    "    dataset = df_spark.toPandas()\n",
    "    desc = dataset.describe().round(2).T\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, figure_size_height))\n",
    "    new_order = ['min', '25%', '50%', '75%', 'max', 'mean', 'count', 'std']\n",
    "    desc = desc[new_order]\n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    col_widths = [0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08]\n",
    "\n",
    "    data_table = ax.table(\n",
    "        cellText=desc.values,\n",
    "        colLabels=desc.columns,\n",
    "        rowLabels=[name[:30] for name in desc.index],\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=col_widths,\n",
    "    )\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(font_size)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.title(f\"Descriptive Statistics {dataset_name}\")\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def du_data_exploration_basics(\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        metric_name_plot: str,\n",
    "        metric_label: str,\n",
    "        dataset_name: str,\n",
    "        country_label: str = 'country',\n",
    "        prefix_file_name: str = 'du',\n",
    "        year_label: str = 'year',\n",
    "):\n",
    "\n",
    "    metric_name_file = '_'.join([word[0:2] for word in metric_label.split('_')])\n",
    "    base_name_file = f'{prefix_file_name}_{dataset_name}_{metric_name_file}'\n",
    "    base_name_file_with_path = f'{PATH_IMAGES}{base_name_file}'\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(20, 10), nrows=2, ncols=1)\n",
    "    grouped_data = df_spark.groupBy(year_label).agg(F.collect_list(metric_label).alias(\"values\"))\n",
    "    data_collected = grouped_data.collect()\n",
    "\n",
    "    years = [row[year_label] for row in data_collected]\n",
    "    data_to_plot = [row['values'] for row in data_collected]\n",
    "    \n",
    "    ax1.boxplot(data_to_plot, vert=True, patch_artist=True, labels=years)\n",
    "    ax1.set_title(f'Yearly Spread of {metric_name_plot}')\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel(metric_name_plot)\n",
    "    \n",
    "\n",
    "    average_per_year = df_spark.groupBy(year_label).agg(F.avg(metric_label).alias('avg_metric'))\n",
    "    average_per_year = average_per_year.orderBy(year_label)\n",
    "    data_collected = average_per_year.collect()\n",
    "\n",
    "    years = [row[year_label] for row in data_collected]\n",
    "    avg_values = [row['avg_metric'] for row in data_collected]\n",
    "    \n",
    "    \n",
    "    ax2.plot(years, avg_values, linestyle='-', marker='o', color='b', label=f'Average {metric_name_plot}')\n",
    "    ax2.set_title(f'Average {metric_name_plot} Over Years')\n",
    "    ax2.set_ylabel(metric_name_plot)\n",
    "    ax2.set_xlabel('Year')\n",
    "    \n",
    "    plt.suptitle('')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{base_name_file_with_path}_y_trend.png', bbox_inches='tight', dpi=80)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    data_collected = df_spark.select(metric_label).rdd.flatMap(lambda x: x).collect()\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.hist(data_collected, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of {metric_name_plot}')\n",
    "    plt.xlabel(metric_name_plot)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(f'{base_name_file_with_path}_freq.png', bbox_inches='tight', dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    average_by_country = df_spark.groupBy(country_label).agg(F.avg(metric_label).alias(\"avg_metric\"))\n",
    "    top_countries = average_by_country.orderBy(F.desc(\"avg_metric\")).limit(20)\n",
    "    bottom_countries = average_by_country.orderBy(\"avg_metric\").limit(20)\n",
    "\n",
    "    top_countries_list = [row[country_label] for row in top_countries.collect()]\n",
    "    bottom_countries_list = [row[country_label] for row in bottom_countries.collect()]\n",
    "\n",
    "    top_countries_data = df_spark.filter(df_spark[country_label].isin(top_countries_list))\n",
    "    bottom_countries_data = df_spark.filter(df_spark[country_label].isin(bottom_countries_list))\n",
    "\n",
    "    top_countries_pd = top_countries_data.toPandas()\n",
    "    bottom_countries_pd = bottom_countries_data.toPandas()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(18, 12))\n",
    "    fig.tight_layout(pad=10.0)\n",
    "\n",
    "    top_countries_pd.boxplot(column=metric_label, by=country_label, ax=ax1, rot=80)\n",
    "    ax1.set_title(f'{metric_name_plot} for Top 20 Countries')\n",
    "    ax1.set_xlabel('Country')\n",
    "    ax1.set_ylabel(metric_name_plot)\n",
    "\n",
    "    bottom_countries_pd.boxplot(column=metric_label, by=country_label, ax=ax2, rot=80)\n",
    "    ax2.set_title(f'{metric_name_plot} for Lowest 20 Countries')\n",
    "    ax2.set_xlabel('Country')\n",
    "    ax2.set_ylabel(metric_name_plot)\n",
    "\n",
    "    plt.suptitle('')\n",
    "    plt.savefig(f'{base_name_file_with_path}_cou_t_l_20.png', bbox_inches='tight', dpi=80)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Plotting aggregated data\n",
    "    merged_pd = pd.concat([top_countries_pd, bottom_countries_pd])\n",
    "    avg_values_by_country = merged_pd.groupby(country_label)[metric_label].mean().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    avg_values_by_country.plot(kind='barh', color='skyblue')\n",
    "    plt.title(f'Average {metric_name_plot} by Country 20 top and 20 lowest')\n",
    "    plt.xlabel(metric_name_plot)\n",
    "    plt.ylabel('Country')\n",
    "    plt.savefig(f'{base_name_file_with_path}_cou_t_l_20_v2.png', bbox_inches='tight', dpi=80)\n",
    "    plt.close()\n",
    "\n",
    "def capture_table_dataframe_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        col_widths: List[float],\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "        font_size=4,\n",
    "        head=None,\n",
    "        show_records=True,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    # Convert the Spark DataFrame to a list of lists and retrieve column names\n",
    "    if head:\n",
    "        data_values = df_spark.limit(head).collect()\n",
    "    else:\n",
    "        data_values = df_spark.collect()\n",
    "    columns = df_spark.columns\n",
    "    \n",
    "    # Create table with the collected data and columns\n",
    "    data_table = ax.table(\n",
    "        cellText=data_values,\n",
    "        colLabels=[' '.join(col.split('_')) for col in columns],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "    \n",
    "    num_rows = df_spark.count() - 1\n",
    "    records = f'(Records: {num_rows})' if show_records else ''\n",
    "    plt.title(f'{table_name} {records}')\n",
    "    \n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(font_size)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def rename_columns(\n",
    "    df: pyspark.sql.dataframe.DataFrame,\n",
    "    map_columns: Dict[str,str]\n",
    ") -> pyspark.sql.dataframe.DataFrame:\n",
    "    for old_name, new_name in map_columns.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    return df\n",
    "\n",
    "def read_csv(file_path_enum: DataFramesCSV, sep=\",\") -> pyspark.sql.dataframe.DataFrame:\n",
    "    return spark.read.csv(file_path_enum.value, header=True, inferSchema=True, sep=sep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crisp Process Manager\n",
    "\n",
    "I will create a manager that will make easier to follow each steps for the reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CRISPManager:\n",
    "    missing_countries = None\n",
    "    country_master = None\n",
    "    integrated_dataset = None\n",
    "    generate_images_du_02: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        country_master = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)\n",
    "        country_master = rename_columns(\n",
    "            df=country_master,\n",
    "            map_columns={\n",
    "                \"alpha-3\": \"country_code\",\n",
    "                \"name\": \"country\"\n",
    "            }\n",
    "        )\n",
    "        self.country_master = country_master\n",
    "\n",
    "    def _capture_get_dataframe_info_image(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            name_file: str,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            figure_size_height=10.0,\n",
    "            figure_size_width=5.0,\n",
    "            force_save_image=False,\n",
    "            previous_data = None,\n",
    "            previous_data_name: DataFramePreviousFieldNameOptions = None,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_get_dataframe_info_image(\n",
    "                table_name=table_name,\n",
    "                name_file=name_file,\n",
    "                df_spark=df_spark,\n",
    "                figure_size_height=figure_size_height,\n",
    "                figure_size_width=figure_size_width,\n",
    "                previous_data_name=previous_data_name,\n",
    "                previous_data=previous_data,\n",
    "            )\n",
    "    \n",
    "    def _capture_summary_dataset_to_image(\n",
    "            self,\n",
    "            name_file: str,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            dataset_name: str,\n",
    "            figure_size_height=3,\n",
    "            font_size=7,\n",
    "            force_save_image=False,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_summary_dataset_to_image(\n",
    "                name_file=name_file,\n",
    "                df_spark=df_spark,\n",
    "                dataset_name=dataset_name,\n",
    "                figure_size_height=figure_size_height,\n",
    "                font_size=font_size,\n",
    "            )\n",
    "\n",
    "    def _capture_table_dataframe_image(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            name_file: str,\n",
    "            col_widths: List[float],\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            figure_size_height=10.0,\n",
    "            figure_size_width=5.0,\n",
    "            font_size=4,\n",
    "            head=None,\n",
    "            force_save_image=False,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_table_dataframe_image(\n",
    "                table_name=table_name,\n",
    "                name_file=name_file,\n",
    "                col_widths=col_widths,\n",
    "                df_spark=df_spark,\n",
    "                figure_size_width=figure_size_width,\n",
    "                figure_size_height=figure_size_height,\n",
    "                font_size=font_size,\n",
    "                head=head\n",
    "            )\n",
    "\n",
    "\n",
    "    def _du_data_exploration_basics(\n",
    "            self,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            metric_name_plot: str,\n",
    "            metric_label: str,\n",
    "            dataset_name: str,\n",
    "            country_label: str = 'country',\n",
    "            prefix_file_name: str = 'du',\n",
    "            year_label: str = 'year',\n",
    "            force_save_image=False,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            du_data_exploration_basics(\n",
    "                dataset_name=dataset_name,\n",
    "                df_spark=df_spark,\n",
    "                metric_label=metric_label,\n",
    "                metric_name_plot=metric_name_plot,\n",
    "                country_label=country_label,\n",
    "                prefix_file_name=prefix_file_name,\n",
    "                year_label=year_label,\n",
    "            )\n",
    "    \n",
    "    def _merge_by_country_code(self, dataset_name, target_dataset):\n",
    "        # 2. Full join on the \"id\" column\n",
    "        full_joined = target_dataset.join(self.country_master, \"country_code\", \"left\")\n",
    "        not_found = full_joined.filter(F.col(\"country-code\").isNull()).select(\"country_code\").distinct()\n",
    "        \n",
    "        not_found = not_found.withColumn(\n",
    "            \"dataset\", F.lit(dataset_name)\n",
    "        ).withColumn(\n",
    "            \"value\", F.lit(1)\n",
    "        ).withColumn(\n",
    "            \"replacement\", F.lit(None)\n",
    "        )\n",
    "        not_found = rename_columns(\n",
    "            df=not_found,\n",
    "            map_columns={'country_code':'missing'}\n",
    "        )\n",
    "        if self.missing_countries is None:\n",
    "            self.missing_countries = not_found\n",
    "        else:\n",
    "            self.missing_countries = not_found.union(self.missing_countries)\n",
    "\n",
    "    def _merge_by_country_name(self, dataset_name, target_dataset):\n",
    "        # 2. Full join on the \"id\" column\n",
    "        full_joined = target_dataset.join(self.country_master, \"country\", \"left\")\n",
    "        not_found = full_joined.filter(F.col(\"country-code\").isNull()).select(\"country\").distinct()\n",
    "        \n",
    "        not_found = not_found.withColumn(\n",
    "            \"dataset\", F.lit(dataset_name)\n",
    "        ).withColumn(\n",
    "            \"value\", F.lit(1)\n",
    "        ).withColumn(\n",
    "            \"replacement\", F.lit(None)\n",
    "        )\n",
    "        not_found = rename_columns(\n",
    "            df=not_found,\n",
    "            map_columns={'country':'missing'}\n",
    "        )\n",
    "        if self.missing_countries is None:\n",
    "            self.missing_countries = not_found\n",
    "        else:\n",
    "            self.missing_countries = not_found.union(self.missing_countries)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _du_02_country_master(self):\n",
    "        country_master = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Countries Dataset',\n",
    "            name_file='du_country_dataset',\n",
    "            df_spark=country_master,\n",
    "            figure_size_height=2.5\n",
    "        )\n",
    "\n",
    "    def _du_02_meat_consumption(self):\n",
    "        meat_consumption = read_csv(DataFramesCSV.MEAT_CONSUMPTION_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Meat Consumption Dataset',\n",
    "            name_file='du_meat_consumption_dataset',\n",
    "            df_spark=meat_consumption,\n",
    "            figure_size_height=2\n",
    "        )\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=meat_consumption,\n",
    "            dataset_name='Meat Consumption',\n",
    "            name_file='du_meat_consumption_summary',\n",
    "            figure_size_height=2\n",
    "        )\n",
    "\n",
    "        meat_consumption = rename_columns(\n",
    "            df=meat_consumption,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.MEAT_CONSUMPTION_CSV)\n",
    "        )\n",
    "        self._merge_by_country_code(dataset_name='meat_consumption', target_dataset=meat_consumption)\n",
    "\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Beef consumption',\n",
    "            metric_label='beef',\n",
    "            dataset_name='meat_beef',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Poultry consumption',\n",
    "            metric_label='poultry',\n",
    "            dataset_name='meat_poultry',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Sheep and Goat consumption',\n",
    "            metric_label='sheep_and_goat',\n",
    "            dataset_name='meat_sheep',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption.fillna(0),\n",
    "            metric_name_plot='Kg./Year per Capita - Pig consumption',\n",
    "            metric_label='pig',\n",
    "            dataset_name='meat_pig',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Fish and Seafood consumption',\n",
    "            metric_label='fish_and_seafood',\n",
    "            dataset_name='meat_fish_seafood',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "\n",
    "    def _du_02_hunger(self):\n",
    "        hunger = read_csv(DataFramesCSV.HUNGER_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Hunger Dataset',\n",
    "            name_file='du_hunger_dataset',\n",
    "            df_spark=hunger,\n",
    "            figure_size_height=1.5\n",
    "        )\n",
    "        \n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=hunger,\n",
    "            dataset_name='Hunger',\n",
    "            name_file='du_hunger_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        hunger = hunger.select(\"Entity\", \"Year\", \"Global Hunger Index (2021)\")\n",
    "        hunger = rename_columns(\n",
    "            df=hunger,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.HUNGER_CSV)\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=hunger,\n",
    "            metric_name_plot='Global Hunger Index',\n",
    "            metric_label='hunger_index',\n",
    "            dataset_name='hunger',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='hunger', target_dataset=hunger)\n",
    "\n",
    "    def _du_02_smoking(self):\n",
    "        smoking = read_csv(DataFramesCSV.SMOKING_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Smoking Dataset',\n",
    "            name_file='du_smoking_dataset',\n",
    "            df_spark=smoking,\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        \n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=smoking,\n",
    "            dataset_name='Smoking',\n",
    "            name_file='du_smoking_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        smoking = smoking.drop(\"Code\")        \n",
    "        smoking = rename_columns(\n",
    "            df=smoking,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.SMOKING_CSV)\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=smoking,\n",
    "            metric_name_plot='Percentage Prevalence Tobacco use Adults',\n",
    "            metric_label='prevalence_smoking',\n",
    "            dataset_name='smoking',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='smoking', target_dataset=smoking)\n",
    "\n",
    "    def _du_02_alcohol_consumption(self):\n",
    "        alcohol_consumption = read_csv(DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Alcohol Consumption Dataset',\n",
    "            name_file='du_alcohol_consumption_dataset',\n",
    "            df_spark=alcohol_consumption,\n",
    "            figure_size_height=1.2\n",
    "        )\n",
    "        \n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=alcohol_consumption,\n",
    "            dataset_name='Alcohol Consumption',\n",
    "            name_file='du_alcohol_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        alcohol_consumption = alcohol_consumption.drop(\"Code\")        \n",
    "        alcohol_consumption = rename_columns(\n",
    "            df=alcohol_consumption,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=alcohol_consumption,\n",
    "            metric_name_plot='Liters of Pure Alcohol per Capita',\n",
    "            metric_label='liters_of_pure_alcohol_per_capita',\n",
    "            dataset_name='alcohol',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='alcohol_consumption', target_dataset=alcohol_consumption)\n",
    "    \n",
    "    def _du_02_obesity(self):\n",
    "        obesity_dataset = read_csv(DataFramesCSV.WHO_OBESITY_CSV)\n",
    "\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=obesity_dataset,\n",
    "            dataset_name='obesity',\n",
    "            name_file='du_obesity_summary'\n",
    "        )\n",
    "\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Obesity Dataset',\n",
    "            name_file='du_obesity_dataset',\n",
    "            df_spark=obesity_dataset,\n",
    "            figure_size_height=3.3\n",
    "        )\n",
    "\n",
    "        # Filter the DataFrame based on the 'Sex' column\n",
    "        obesity_dataset = obesity_dataset.filter(obesity_dataset.Sex == \"Both sexes\")\n",
    "\n",
    "        # Select specific columns\n",
    "        obesity_dataset = obesity_dataset.select(\"Numeric\", \"Countries, territories and areas\", \"WHO region\", \"Year\")\n",
    "        \n",
    "        obesity_dataset= rename_columns(\n",
    "            df=obesity_dataset,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.WHO_OBESITY_CSV)\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='obesity', target_dataset=obesity_dataset)\n",
    "\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=obesity_dataset.fillna(0),\n",
    "            metric_name_plot='Percentage Obesity',\n",
    "            metric_label='percentage_obesity',\n",
    "            dataset_name='obesity',\n",
    "        )\n",
    "\n",
    "\n",
    "    def _du_02_happiness(self):\n",
    "        happiness_record = read_csv(DataFramesCSV.HAPPINESS_REPORT_CSV)\n",
    "        \n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Happiness Report Dataset',\n",
    "            name_file='du_happiness_dataset',\n",
    "            df_spark=happiness_record,\n",
    "            figure_size_height=2.5\n",
    "        )\n",
    "\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=happiness_record,\n",
    "            dataset_name='Happiness',\n",
    "            name_file='du_happiness_summary',\n",
    "            figure_size_height=2.3\n",
    "        )\n",
    "        map_columns_happiness = {col: col.split(',', 2)[-1].strip() for col in happiness_record.columns}\n",
    "        \n",
    "        happiness_record = rename_columns(\n",
    "            df=happiness_record,\n",
    "            map_columns=map_columns_happiness\n",
    "        )\n",
    "        \n",
    "        happiness_record = rename_columns(\n",
    "            df=happiness_record,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.HAPPINESS_REPORT_CSV)\n",
    "        )\n",
    "        \n",
    "        self._merge_by_country_name(dataset_name='happiness', target_dataset=happiness_record)\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record,\n",
    "            metric_name_plot='Life Ladder',\n",
    "            metric_label='life_ladder',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Social Support',\n",
    "            metric_label='social_support',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Freedom to Make Life Choices',\n",
    "            metric_label='freedom_to_make_life_choices',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Generosity',\n",
    "            metric_label='generosity',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Perceptions of Corruption',\n",
    "            metric_label='perceptions_of_corruption',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Positive Affect',\n",
    "            metric_label='positive_affect',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Negative affect',\n",
    "            metric_label='negative_affect',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "\n",
    "    def _function_mapper_du_02(self, dataset: DataFramesCSV):\n",
    "        mapper = {\n",
    "            DataFramesCSV.MEAT_CONSUMPTION_CSV: self._du_02_meat_consumption,\n",
    "            DataFramesCSV.WHO_OBESITY_CSV: self._du_02_obesity,\n",
    "            DataFramesCSV.HAPPINESS_REPORT_CSV: self._du_02_happiness,\n",
    "            DataFramesCSV.HUNGER_CSV: self._du_02_hunger,\n",
    "            DataFramesCSV.SMOKING_CSV: self._du_02_smoking,\n",
    "            DataFramesCSV.ALCOHOL_CONSUMPTION_CSV: self._du_02_alcohol_consumption,\n",
    "        }\n",
    "\n",
    "        return mapper.get(dataset)\n",
    "    \n",
    "    def get_crosstab_missing_countries(self, without_replacement=False, save_table=False, head=20):\n",
    "        if without_replacement:\n",
    "            filtered_dataframe = self.missing_countries.filter(self.missing_countries[\"replacement\"].isNull())\n",
    "        else:\n",
    "            filtered_dataframe = self.missing_countries\n",
    "        \n",
    "        # Compute the crosstab\n",
    "        crosstab_result = filtered_dataframe.crosstab(\"missing\", \"dataset\")\n",
    "        \n",
    "        # Optionally save the table\n",
    "        if save_table:\n",
    "            if head:\n",
    "                file_name = f'du_missing_countries_per_dataset_head_{head}'\n",
    "            else:\n",
    "                file_name = 'du_missing_countries_per_dataset'\n",
    "            self._capture_table_dataframe_image(\n",
    "                table_name='Missing countries by datasets (20 firsts)',\n",
    "                df_spark=crosstab_result,\n",
    "                col_widths=[0.35, 0.2, 0.1, 0.1, 0.2, 0.1, 0.1, 0.07],\n",
    "                name_file=file_name,\n",
    "                head=head,\n",
    "                font_size=5,\n",
    "                figure_size_height=4.5,\n",
    "            )\n",
    "        \n",
    "        return crosstab_result\n",
    "    \n",
    "    def _generate_missing_countries_replacement_table(self):\n",
    "        # Remove groups of countries\n",
    "        df_filtered = self.missing_countries.filter(~F.col(\"missing\").startswith('OWID'))\n",
    "        df_filtered = df_filtered.filter(~F.col(\"missing\").rlike('WB'))\n",
    "        df_filtered = df_filtered.filter(~F.col(\"missing\").rlike('income'))\n",
    "\n",
    "        # Remove some that are not countries, or have been dissolved, or are present in just one database\n",
    "        excluded_countries = [\n",
    "            'European Union (27)',\n",
    "            'World',\n",
    "            'Kinshasa', # This small country is not present in the others datasets\n",
    "            'Somaliland region',\n",
    "            'Kosovo',  # Present in just one database\n",
    "            'Sudan (former)',  # Present in just one database\n",
    "            'ANT',  # Was dissolved in 2010\n",
    "            'CIV',  # Present in just one database\n",
    "        ]\n",
    "        df_filtered = df_filtered.filter(~F.col(\"missing\").isin(excluded_countries))\n",
    "\n",
    "        missing_replacement = [\n",
    "            ('Bosnia and Herzegovina', 'Bosnia And Herzegovina'),\n",
    "            ('Czechia', 'Czech Republic'),\n",
    "            ('North Macedonia', 'Macedonia'),\n",
    "            ('Guinea-Bissau', 'Guinea Bissau'),\n",
    "            ('Cape Verde', 'Cabo Verde'),\n",
    "            ('East Timor', 'Timor-Leste'),\n",
    "            ('North Korea', 'Korea'),\n",
    "            ('Democratic Republic of Congo', 'Congo (Democratic Republic Of The)'),\n",
    "            ('Brunei', 'Brunei Darussalam'),\n",
    "            ('Viet Nam', 'Vietnam'),\n",
    "            ('Hong Kong S.A.R. of China', 'Hong Kong'),\n",
    "            (\"Lao People's Democratic Republic\", 'Laos'),\n",
    "            ('United States of America', 'United States'),\n",
    "            ('Democratic Republic of the Congo', 'Congo (Democratic Republic Of The)'),\n",
    "            ('Congo (Brazzaville)', 'Congo'),\n",
    "            ('Syrian Arab Republic', 'Syria'),\n",
    "            ('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom'),\n",
    "            ('Bolivia (Plurinational State of)', 'Bolivia'),\n",
    "            ('Ethiopia (former)', 'Ethiopia'),\n",
    "            ('State of Palestine', 'Palestine'),\n",
    "            ('Taiwan Province of China', 'Taiwan'),\n",
    "            ('Turkiye', 'Turkey'),\n",
    "            ('Türkiye', 'Turkey'),\n",
    "            (\"Democratic People's Republic of Korea\", 'Korea'),\n",
    "            ('United Republic of Tanzania', 'Tanzania'),\n",
    "            ('Iran (Islamic Republic of)', 'Iran'),\n",
    "            ('Republic of Moldova', 'Moldova'),\n",
    "            ('Venezuela (Bolivarian Republic of)', 'Venezuela'),\n",
    "            ('Russian Federation', 'Russia'),\n",
    "            ('Micronesia (country)', 'Micronesia (Federated States of)'),\n",
    "            ('Republic of Korea', 'Korea'),\n",
    "            (\"Cote de Ivoire\", \"Côte D'Ivoire\"),\n",
    "            (\"Cote d'Ivoire\", \"Côte D'Ivoire\"),\n",
    "            (\"Côte d'Ivoire\", \"Côte D'Ivoire\"),\n",
    "            ('Ivory Coast', \"Côte D'Ivoire\"),\n",
    "        ]\n",
    "\n",
    "        for missing, replacement in missing_replacement:\n",
    "            df_filtered = df_filtered.withColumn(\"replacement\", F.when(F.col(\"missing\") == missing, F.lit(replacement)).otherwise(F.col(\"replacement\")))\n",
    "\n",
    "        self.missing_countries = df_filtered\n",
    "        return df_filtered\n",
    "    \n",
    "    def _du_03_filter_data(self, df_enum_path, dataframe):\n",
    "        if df_enum_path == DataFramesCSV.WHO_OBESITY_CSV:\n",
    "            return dataframe.filter(\n",
    "                (F.col(\"Sex\") == \"Both sexes\") & \n",
    "                (F.col(\"year\") >= 2000) & \n",
    "                (F.col(\"year\") <= 2016)\n",
    "            )\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    def _find_missing_countries(self, target_dataset_path, merged_by_in_country_master=\"country\"):\n",
    "        missing_countries = self.missing_countries.withColumnRenamed(\"missing\", merged_by_in_country_master)\n",
    "\n",
    "        target_dataset = read_csv(target_dataset_path)\n",
    "        \n",
    "        target_dataset = rename_columns(\n",
    "            df=target_dataset,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(target_dataset_path)\n",
    "        )\n",
    "\n",
    "        target_dataset = self._du_03_filter_data(df_enum_path=target_dataset_path, dataframe=target_dataset)\n",
    "\n",
    "        desired_columns = [column for column in COLUMN_RENAME_BY_DATASET.get(target_dataset_path).values()]\n",
    "        columns_to_remove = [column for column in target_dataset.columns if column not in desired_columns]\n",
    "\n",
    "        for col_name in columns_to_remove:\n",
    "            target_dataset = target_dataset.drop(col_name)\n",
    "\n",
    "        country_master = self.country_master.select(\"country\", \"country_code\")\n",
    "        existing = target_dataset.join(country_master, on=merged_by_in_country_master, how=\"inner\")\n",
    "        \n",
    "\n",
    "        merged_missing = target_dataset.join(missing_countries, on=merged_by_in_country_master, how=\"left\")\n",
    "        \n",
    "        \n",
    "        if 'country' not in merged_missing.columns:\n",
    "            merged_missing = merged_missing.withColumn('country', F.lit(None))\n",
    "        \n",
    "        merged_missing = merged_missing.withColumn(\n",
    "            \"country\",\n",
    "            F.when(F.col(\"replacement\").isNotNull(),\n",
    "            F.col(\"replacement\")).otherwise(F.col(\"country\"))\n",
    "        )\n",
    "        merged_missing = merged_missing.drop(\"replacement\")\n",
    "        \n",
    "        merged_missing = merged_missing.filter(F.col(\"country\").isNotNull())\n",
    "\n",
    "        mutual_columns = list(set(existing.columns) & set(merged_missing.columns))        \n",
    "        merged_missing = merged_missing.select(*mutual_columns)\n",
    "        existing = existing.select(*mutual_columns)\n",
    "\n",
    "        full_countries = existing.union(merged_missing)\n",
    "\n",
    "        columns_to_remove = [column for column in full_countries.columns if column.startswith(\"country_code\")]\n",
    "        for col_name in columns_to_remove:\n",
    "            full_countries = full_countries.drop(col_name)\n",
    "        \n",
    "        return full_countries.dropDuplicates()\n",
    "    \n",
    "    def _dp_03_generate_merged_dataset(self):\n",
    "        merged_dataset = self._find_missing_countries(\n",
    "            target_dataset_path=DataFramesCSV.WHO_OBESITY_CSV\n",
    "        )\n",
    "\n",
    "        for df_enum_path in [\n",
    "            DataFramesCSV.HUNGER_CSV,\n",
    "            DataFramesCSV.SMOKING_CSV,\n",
    "            DataFramesCSV.ALCOHOL_CONSUMPTION_CSV,\n",
    "            DataFramesCSV.MEAT_CONSUMPTION_CSV,\n",
    "            DataFramesCSV.HAPPINESS_REPORT_CSV,\n",
    "        ]:\n",
    "            is_meat_consumption_dataset = df_enum_path == DataFramesCSV.MEAT_CONSUMPTION_CSV\n",
    "            country_column = 'country_code' if is_meat_consumption_dataset else 'country'\n",
    "            dataset = self._find_missing_countries(\n",
    "                target_dataset_path=df_enum_path,\n",
    "                merged_by_in_country_master=country_column\n",
    "            )\n",
    "            merged_dataset = merged_dataset.join(\n",
    "                dataset,\n",
    "                on=['country', 'year'],\n",
    "                how='left_outer'\n",
    "            )\n",
    "            merged_dataset = merged_dataset.drop('country-code')\n",
    "\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Merged Dataset 1',\n",
    "            name_file='dp_merged_dataset_01',\n",
    "            df_spark=merged_dataset,\n",
    "            figure_size_height=4.3,\n",
    "        )\n",
    "        self.integrated_dataset = merged_dataset\n",
    "\n",
    "    def _du_03_remove_rows(self):\n",
    "        self.integrated_dataset = self.integrated_dataset.filter(\n",
    "            ~(\n",
    "                F.col(\"prevalence_smoking\").isNull() &\n",
    "                F.col(\"liters_of_pure_alcohol_per_capita\").isNull() &\n",
    "                F.col(\"poultry\").isNull() &\n",
    "                F.col(\"beef\").isNull() &\n",
    "                F.col(\"sheep_and_goat\").isNull() &\n",
    "                F.col(\"pig\").isNull() &\n",
    "                F.col(\"fish_and_seafood\").isNull() &\n",
    "                F.col(\"life_ladder\").isNull() &\n",
    "                F.col(\"social_support\").isNull() &\n",
    "                F.col(\"freedom_to_make_life_choices\").isNull() &\n",
    "                F.col(\"generosity\").isNull() &\n",
    "                F.col(\"perceptions_of_corruption\").isNull() &\n",
    "                F.col(\"positive_affect\").isNull() &\n",
    "                F.col(\"negative_affect\").isNull()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _dp_03_create_expected_obesity_rate_attribute(self) -> None:\n",
    "        threshold_in_hunger = 20\n",
    "        other_countries_in_hunger = ['Burundi', 'Guinea', 'Libya', 'Niger', 'Uganda', 'Zambia', 'Zimbabwe']\n",
    "\n",
    "        df = self.integrated_dataset\n",
    "        df = df.withColumn(\n",
    "            \"expected_obesity_rate\", \n",
    "            F.when(df[\"percentage_obesity\"] <= 20, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"in_hunger\", \n",
    "            F.when(df[\"hunger_index\"] >= threshold_in_hunger, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"in_hunger\",\n",
    "            F.when(df[\"country\"].isin(other_countries_in_hunger), 1).otherwise(df[\"in_hunger\"])\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"expected_obesity_rate\",\n",
    "            F.when((df[\"expected_obesity_rate\"] == 1) & (df[\"in_hunger\"] == 1), 0).otherwise(df[\"expected_obesity_rate\"])\n",
    "        )\n",
    "\n",
    "        df = df.drop(\"in_hunger\", \"percentage_obesity\", \"hunger_index\")\n",
    "        \n",
    "        self.integrated_dataset = df\n",
    "\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='New Feature: Obesity Rate Attribute',\n",
    "            name_file='dt_new_feat_obes_rate_attribute',\n",
    "            df_spark=df,\n",
    "            figure_size_height=4.3,\n",
    "            force_save_image=True\n",
    "        )\n",
    "\n",
    "    def _du_03_impute_missing_grouped_country_year(self):\n",
    "        print(\"Imputing by grouped by year\")\n",
    "        happiness_columns = [\n",
    "            'life_ladder',\n",
    "            'social_support',\n",
    "            'freedom_to_make_life_choices',\n",
    "            'generosity',\n",
    "            'perceptions_of_corruption',\n",
    "            'positive_affect',\n",
    "            'negative_affect'\n",
    "        ]\n",
    "        others_columns = [\n",
    "            'hunger_index',\n",
    "            'prevalence_smoking',\n",
    "            'liters_of_pure_alcohol_per_capita',\n",
    "\n",
    "        ]\n",
    "        meat_columns = [\n",
    "            'poultry',\n",
    "            'beef',\n",
    "            'sheep_and_goat',\n",
    "            'pig',\n",
    "            'fish_and_seafood',\n",
    "        ]\n",
    "        \n",
    "        windowSpec = window.Window.partitionBy(\"country\").orderBy(\"year\")\n",
    "        \n",
    "        df = self.integrated_dataset\n",
    "\n",
    "        for column in happiness_columns + others_columns + meat_columns:\n",
    "            print(f\"Imputing {column}\")\n",
    "            old_null_values = (\n",
    "                self.integrated_dataset.select(\n",
    "                    *[F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in self.integrated_dataset.columns]\n",
    "                ).first()\n",
    "            )\n",
    "            old_null_values_list = list(old_null_values)\n",
    "            df = df.withColumn(\n",
    "                column, F.coalesce(df[column], F.last(df[column], True).over(windowSpec))\n",
    "            )\n",
    "            df = df.withColumn(\n",
    "                column, F.coalesce(df[column], F.first(df[column], True).over(windowSpec))\n",
    "            )\n",
    "            self.integrated_dataset = df\n",
    "\n",
    "            self._capture_get_dataframe_info_image(\n",
    "                table_name=f'Imputation: By Group Country Year: {column}',\n",
    "                name_file=f'dp_imputation_c_y_{column}',\n",
    "                df_spark=self.integrated_dataset,\n",
    "                figure_size_height=4.3,\n",
    "                previous_data=old_null_values_list,\n",
    "                previous_data_name=DataFramePreviousFieldNameOptions.IS_NULL,\n",
    "            )\n",
    "\n",
    "    def _du_03_impute_missing_pig(self):        \n",
    "        old_null_values = self.integrated_dataset.select([F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in self.integrated_dataset.columns]).collect()[0].asDict()\n",
    "        old_null_values_list = list(old_null_values.values())\n",
    "\n",
    "        condition = (~F.col('beef').isNull()) & (~F.col('poultry').isNull()) & (F.col('pig').isNull())\n",
    "        \n",
    "        self.integrated_dataset = self.integrated_dataset.withColumn(\n",
    "            \"pig\",\n",
    "            F.when(condition, 0).otherwise(F.col(\"pig\"))\n",
    "        )\n",
    "\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Imputation: Pig',\n",
    "            name_file='dp_imput_pig',\n",
    "            df_spark=self.integrated_dataset,\n",
    "            figure_size_height=4.3,\n",
    "            previous_data=old_null_values_list,\n",
    "            previous_data_name=DataFramePreviousFieldNameOptions.IS_NULL,\n",
    "        )\n",
    "    \n",
    "    def _du_03_impute_missing_important_features(self):\n",
    "        old_null_values = (\n",
    "            self.integrated_dataset.select(\n",
    "                *[F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in self.integrated_dataset.columns]\n",
    "            )\n",
    "            .first()\n",
    "        )\n",
    "        old_null_values_list = list(old_null_values)\n",
    "\n",
    "        self.integrated_dataset = self.integrated_dataset.filter(\n",
    "            F.col('pig').isNotNull() & F.col('life_ladder').isNotNull()\n",
    "        )\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Imputation: Rows Without Important Features',\n",
    "            name_file='dp_imput_impor_feat',\n",
    "            df_spark=self.integrated_dataset,\n",
    "            figure_size_height=4.3,\n",
    "            previous_data=old_null_values,\n",
    "            previous_data_name=DataFramePreviousFieldNameOptions.IS_NULL,\n",
    "        )\n",
    "\n",
    "    def _du_03_impute_by_prediction(self, columns_to_impute: List[str]):\n",
    "        columns_to_drop = ['percentage_obesity', 'country', 'region', 'hunger_index', 'year'] + columns_to_impute\n",
    "\n",
    "        print(\"Imputing by predictions\")\n",
    "        for feature in columns_to_impute:\n",
    "            print(f\"Imputing {feature}\")\n",
    "            old_null_values = (\n",
    "                self.integrated_dataset.select(\n",
    "                    *[F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in self.integrated_dataset.columns]\n",
    "                ).first()\n",
    "            )\n",
    "            old_null_values_list = list(old_null_values)\n",
    "            input_cols = [col for col in self.integrated_dataset.columns if col not in columns_to_drop]\n",
    "\n",
    "            assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "            rf = RandomForestRegressor(featuresCol=\"features\", labelCol=feature)\n",
    "\n",
    "            train_data = self.integrated_dataset.filter(F.col(feature).isNotNull())\n",
    "            test_data = self.integrated_dataset.filter(F.col(feature).isNull())\n",
    "            \n",
    "            train_data_ass = assembler.transform(train_data)\n",
    "            model = rf.fit(train_data_ass)\n",
    "            \n",
    "            \n",
    "            test_data_ass = assembler.transform(test_data)\n",
    "            # predictions = model.transform(test_data_ass)\n",
    "            test_data = test_data.withColumn(\"uid\", F.monotonically_increasing_id())\n",
    "            predictions = model.transform(test_data_ass).withColumn(\"uid\", F.monotonically_increasing_id())\n",
    "            \n",
    "            test_data = test_data.join(\n",
    "                predictions.select(\"prediction\",\"uid\"), on=\"uid\", how=\"left\"\n",
    "            ).drop(\"uid\", feature).withColumnRenamed(\"prediction\", feature)\n",
    "            \n",
    "            self.integrated_dataset = train_data.union(test_data)\n",
    "\n",
    "            self._capture_get_dataframe_info_image(\n",
    "                table_name=f'Imputation by Prediction: {feature.capitalize()}',\n",
    "                name_file=f'dp_imput_{feature}',\n",
    "                df_spark=self.integrated_dataset,\n",
    "                figure_size_height=4.3,\n",
    "                previous_data=old_null_values_list,\n",
    "                previous_data_name=DataFramePreviousFieldNameOptions.IS_NULL,\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def _save_csv_file(self, df, name, rw=False):\n",
    "        if not os.path.exists(CSV_GENERATED):\n",
    "            os.makedirs(CSV_GENERATED)\n",
    "\n",
    "        name = f'{CSV_GENERATED}/{name}.csv'\n",
    "        if not os.path.exists(name):\n",
    "            df.write.csv(name, header=True)                \n",
    "        else:\n",
    "            if rw:\n",
    "                df.write.csv(name, header=True, mode=\"overwrite\")\n",
    "            else:\n",
    "                print(f\"The directory or file {name} already exists!\")\n",
    "    \n",
    "    def _get_csv_files(self, name):\n",
    "        name = f'{CSV_GENERATED}/{name}.csv'\n",
    "        if os.path.exists(name):\n",
    "            return spark.read.csv(name, header=True, inferSchema=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _save_dp_03_ds(self, df, rw=True):\n",
    "        self._save_csv_file(name='DP_03', df=df, rw=rw)\n",
    "    \n",
    "    def _get_dp_03_ds(self):\n",
    "        return self._get_csv_files(name='DP_03')\n",
    "        \n",
    "\n",
    "    def _du_03_impute_missing(self):\n",
    "        self._du_03_impute_missing_grouped_country_year()\n",
    "        self._du_03_impute_missing_pig()\n",
    "        self._du_03_impute_missing_important_features()\n",
    "        self._du_03_impute_by_prediction(\n",
    "            columns_to_impute=[\n",
    "                'perceptions_of_corruption',\n",
    "                'prevalence_smoking',\n",
    "                'social_support',\n",
    "                'generosity',\n",
    "                'positive_affect'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def _du_02_run_processes(self):\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.MEAT_CONSUMPTION_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.WHO_OBESITY_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.HAPPINESS_REPORT_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.HUNGER_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.SMOKING_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)()\n",
    "\n",
    "    def du_02(self):\n",
    "        self._du_02_country_master()\n",
    "        self._du_02_run_processes()\n",
    "        self.get_crosstab_missing_countries(save_table=True)\n",
    "\n",
    "    def dp_03(self):\n",
    "        self._generate_missing_countries_replacement_table()\n",
    "        self._dp_03_generate_merged_dataset()\n",
    "        self._du_03_remove_rows()\n",
    "        self._du_03_impute_missing()\n",
    "        self._dp_03_create_expected_obesity_rate_attribute()\n",
    "    \n",
    "    def _dt_04_get_feature_distribution(self):\n",
    "        # Compute the distribution of the 'expected_obesity_rate' feature\n",
    "        values_counts = self.integrated_dataset.groupby('expected_obesity_rate').count().collect()\n",
    "\n",
    "        # Separate the values and counts for plotting\n",
    "        labels = [row['expected_obesity_rate'] for row in values_counts]\n",
    "        counts = [row['count'] for row in values_counts]\n",
    "\n",
    "        # Plot the distribution\n",
    "        plt.bar(labels, counts, color=['skyblue', 'salmon'])\n",
    "        plt.title('Distribution of Target Label')\n",
    "        plt.xlabel('Label')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=0)  # keep the x-axis labels horizontal\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{PATH_IMAGES}dt_feature_balance.png', dpi=200)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def dt_04(self):\n",
    "        # manager.integrated_dataset = manager._get_csv_files(name=\"DP_03_M6\")\n",
    "        self._dt_04_get_feature_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "manager = CRISPManager(\n",
    "    generate_images_du_02=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager.du_02()\n",
    "# manager.du_03()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "The following method primarily focuses on generating visuals based on various data analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager.du_02()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
