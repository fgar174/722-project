{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "spark = SparkSession.builder.appName('Iteration4').getOrCreate()\n",
    "\n",
    "databases_path = '../datasets/'\n",
    "PATH_IMAGES = '../tex/iterations/iteration_4/images/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFramesCSV(enum.Enum):\n",
    "    ALCOHOL_CONSUMPTION_CSV = f\"{databases_path}4_total-alcohol-consumption-per-capita-litres-of-pure-alcohol.csv\"\n",
    "    COUNTRY_MASTER_CSV = f\"{databases_path}0_master_country_codes.csv\"\n",
    "    WHO_OBESITY_CSV = f\"{databases_path}1_who_obesity.csv\"\n",
    "    MEAT_CONSUMPTION_CSV = f\"{databases_path}2_meat_consumption.csv\"\n",
    "    HUNGER_CSV = f\"{databases_path}5_global_hunger_index.csv\"\n",
    "    SMOKING_CSV = f\"{databases_path}6_share-of-adults-who-smoke.csv\"\n",
    "\n",
    "class DataFramePreviousFieldNameOptions(enum.Enum):\n",
    "    IS_NULL = 'isnull'\n",
    "    D_TYPES = 'dtypes'\n",
    "    COUNT = 'count'\n",
    "\n",
    "class DataFramesXLSX(enum.Enum):\n",
    "    HAPPINESS_REPORT_XLSX = f\"{databases_path}3_happiness_report.xlsx\"\n",
    "\n",
    "\n",
    "COLUMN_RENAME_BY_DATASET = {\n",
    "    DataFramesCSV.WHO_OBESITY_CSV: {\n",
    "        'Numeric': 'percentage_obesity',\n",
    "        'Countries, territories and areas': 'country',\n",
    "        'WHO region': 'region',\n",
    "        'Year': 'year',\n",
    "    },\n",
    "    DataFramesXLSX.HAPPINESS_REPORT_XLSX: {\n",
    "        'year': 'year',\n",
    "        'Country name': 'country',\n",
    "        \"Life Ladder\": 'life_ladder',\n",
    "        \"Social support\": 'social_support',\n",
    "        \"Freedom to make life choices\": \"freedom_to_make_life_choices\",\n",
    "        \"Generosity\": \"generosity\",\n",
    "        \"Perceptions of corruption\": \"perceptions_of_corruption\",\n",
    "        \"Positive affect\": \"positive_affect\",\n",
    "        \"Negative affect\": \"negative_affect\",\n",
    "    },\n",
    "    DataFramesCSV.MEAT_CONSUMPTION_CSV: {\n",
    "        'Code': 'country_code',\n",
    "        'Year': 'year',\n",
    "        \"Meat, poultry | 00002734 || Food available for consumption | 0645pc || kilograms per year per capita\": \"poultry\",\n",
    "        \"Meat, beef | 00002731 || Food available for consumption | 0645pc || kilograms per year per capita\": \"beef\",\n",
    "        \"Meat, sheep and goat | 00002732 || Food available for consumption | 0645pc || kilograms per year per capita\": \"sheep_and_goat\",\n",
    "        \"Meat, pig | 00002733 || Food available for consumption | 0645pc || kilograms per year per capita\": \"pig\",\n",
    "        \"Fish and seafood | 00002960 || Food available for consumption | 0645pc || kilograms per year per capita\": \"fish_and_seafood\",\n",
    "    },\n",
    "    DataFramesCSV.COUNTRY_MASTER_CSV: {\n",
    "        'alpha-3': 'country_code',\n",
    "        'name': 'country'\n",
    "    },\n",
    "    DataFramesCSV.HUNGER_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'Global Hunger Index (2021)': 'hunger_index',\n",
    "    },\n",
    "    DataFramesCSV.SMOKING_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'Prevalence of current tobacco use (% of adults)': 'prevalence_smoking',\n",
    "    },\n",
    "    DataFramesCSV.ALCOHOL_CONSUMPTION_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'liters_of_pure_alcohol_per_capita': 'liters_of_pure_alcohol_per_capita',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def capture_get_dataframe_info_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        previous_data: pd.Series = None,\n",
    "        previous_data_name: DataFramePreviousFieldNameOptions = None,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "):\n",
    "    dataframe = df_spark.toPandas()\n",
    "    info_object = {\n",
    "        'columns': dataframe.columns.str[0:30].tolist(),\n",
    "        'dtypes': dataframe.dtypes.tolist(),\n",
    "        'count': dataframe.count().tolist(),\n",
    "        'isnull': dataframe.isnull().sum().tolist(),\n",
    "    }\n",
    "    dataframe_info = pd.DataFrame(info_object)\n",
    "\n",
    "    if previous_data is not None:\n",
    "        current_data = dataframe_info[previous_data_name.value]\n",
    "        dataframe_info[f'old {previous_data_name.value}'] = previous_data\n",
    "        dataframe_info['change'] = previous_data - current_data\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    dataframe_info.reset_index(inplace=True)\n",
    "    col_widths = [0.08, 0.35, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "    data_table = ax.table(\n",
    "        cellText=dataframe_info.values,\n",
    "        colLabels=[' '.join(col.split('_')) for col in dataframe_info.columns],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    if previous_data is not None:\n",
    "        for (i, j), val in np.ndenumerate(dataframe_info.values):\n",
    "            if j == 6 and val != 0:  # We look into the second column (j==1), and search for zero values\n",
    "                data_table[(i + 1, j)].set_facecolor(\"red\")\n",
    "                data_table[(i + 1, j)].set_text_props(color='white', weight='bold')\n",
    "\n",
    "    num_rows = dataframe.shape[0] - 1\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(6)\n",
    "    plt.title(f'{table_name} (Records: {num_rows})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def capture_get_dataframe_info_image_new(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        previous_data: list = None,\n",
    "        previous_data_name: str = None,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "        PATH_IMAGES=\"./\"\n",
    "):\n",
    "\n",
    "    # Collect required statistics from PySpark DataFrame\n",
    "    column_names = [col[:30] for col in df_spark.columns]\n",
    "    column_types = [dtype for _, dtype in df_spark.dtypes]\n",
    "    row_count = df_spark.count()\n",
    "    column_counts = [row_count for _ in df_spark.columns]\n",
    "    column_null_counts = df_spark.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_spark.columns]).collect()[0]\n",
    "    column_non_null_counts = [row_count - null_count for null_count in column_null_counts]\n",
    "\n",
    "    info_object = {\n",
    "        'columns': column_names,\n",
    "        'dtypes': column_types,\n",
    "        'count': column_non_null_counts,\n",
    "        'isnull': column_null_counts\n",
    "    }\n",
    "\n",
    "    if previous_data is not None:\n",
    "        info_object[f'old {previous_data_name}'] = previous_data\n",
    "        current_data = info_object[previous_data_name]\n",
    "        info_object['change'] = [prev - curr for prev, curr in zip(previous_data, current_data)]\n",
    "\n",
    "    dataframe_info = list(zip(*info_object.values()))\n",
    "\n",
    "    # Plotting the table image\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    col_widths = [0.08, 0.35, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "    data_table = ax.table(\n",
    "        cellText=dataframe_info,\n",
    "        colLabels=[' '.join(col.split('_')) for col in info_object.keys()],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    if previous_data is not None:\n",
    "        for (i, j), val in np.ndenumerate(np.array(dataframe_info)):\n",
    "            if j == 6 and val != 0:  # We look into the last column (j==6), and search for non-zero values\n",
    "                data_table[(i + 1, j)].set_facecolor(\"red\")\n",
    "                data_table[(i + 1, j)].set_text_props(color='white', weight='bold')\n",
    "\n",
    "    num_rows = row_count - 1\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(6)\n",
    "    plt.title(f'{table_name} (Records: {num_rows})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def rename_columns(df, map_columns: Dict[str,str]) -> pyspark.sql.dataframe.DataFrame:\n",
    "    for old_name, new_name in map_columns.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    return df\n",
    "\n",
    "def read_csv(file_path_enum: DataFramesCSV) -> pyspark.sql.dataframe.DataFrame:\n",
    "    return spark.read.csv(file_path_enum.value, header=True, inferSchema=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProjectManager:\n",
    "    missing_countries = None\n",
    "    country_master = None\n",
    "    integrated_dataset = None\n",
    "    generate_images_du_02: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        country_master = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)\n",
    "        country_master = rename_columns(\n",
    "            df=country_master,\n",
    "            map_columns={\n",
    "                \"alpha-3\": \"country_code\",\n",
    "                \"name\": \"country\"\n",
    "            }\n",
    "        )\n",
    "        self.country_master = country_master\n",
    "\n",
    "    def _capture_get_dataframe_info_image(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            name_file: str,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            figure_size_height=10.0,\n",
    "            figure_size_width=5.0,\n",
    "            force_save_image=False,\n",
    "            previous_data: pd.Series = None,\n",
    "            previous_data_name: DataFramePreviousFieldNameOptions = None,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_get_dataframe_info_image(\n",
    "                table_name=table_name,\n",
    "                name_file=name_file,\n",
    "                df_spark=df_spark,\n",
    "                figure_size_height=figure_size_height,\n",
    "                figure_size_width=figure_size_width,\n",
    "                previous_data_name=previous_data_name,\n",
    "                previous_data=previous_data,\n",
    "            )\n",
    "\n",
    "    \n",
    "    def _du_02_country_master(self):\n",
    "        country_master = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Countries Dataset',\n",
    "            name_file='du_country_dataset',\n",
    "            df_spark=country_master,\n",
    "            figure_size_height=2.5\n",
    "        )\n",
    "\n",
    "    def _du_02_meat_consumption(self):\n",
    "        meat_consumption = read_csv(DataFramesCSV.MEAT_CONSUMPTION_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Meat Consumption Dataset',\n",
    "            name_file='du_meat_consumption_dataset',\n",
    "            df_spark=meat_consumption,\n",
    "            figure_size_height=2\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=meat_consumption,\n",
    "            dataset_name='Meat Consumption',\n",
    "            name_file='du_meat_consumption_summary',\n",
    "            figure_size_height=2\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        meat_consumption = rename_columns(\n",
    "            df=meat_consumption,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.MEAT_CONSUMPTION_CSV)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._merge_by_country_code(dataset_name='meat_consumption', target_dataset=meat_consumption)\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Beef consumption',\n",
    "            metric_label='beef',\n",
    "            dataset_name='meat_beef',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Poultry consumption',\n",
    "            metric_label='poultry',\n",
    "            dataset_name='meat_poultry',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Sheep and Goat consumption',\n",
    "            metric_label='sheep_and_goat',\n",
    "            dataset_name='meat_sheep',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Pig consumption',\n",
    "            metric_label='pig',\n",
    "            dataset_name='meat_pig',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Fish and Seafood consumption',\n",
    "            metric_label='fish_and_seafood',\n",
    "            dataset_name='meat_fish_seafood',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "    def _du_02_hunger(self):\n",
    "        hunger = read_csv(DataFramesCSV.HUNGER_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Hunger Dataset',\n",
    "            name_file='du_hunger_dataset',\n",
    "            df_spark=hunger,\n",
    "            figure_size_height=1.5\n",
    "        )\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=hunger,\n",
    "            dataset_name='Hunger',\n",
    "            name_file='du_hunger_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        hunger = hunger[[\"Entity\", \"Year\", \"Global Hunger Index (2021)\"]]\n",
    "        hunger.rename(\n",
    "            inplace=True,\n",
    "            columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.HUNGER_CSV)\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        self._merge_by_country_name(dataset_name='hunger', target_dataset=hunger)\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=hunger,\n",
    "            metric_name_plot='Global Hunger Index',\n",
    "            metric_label='hunger_index',\n",
    "            dataset_name='hunger',\n",
    "            country_label='country',\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "    def _du_02_smoking(self):\n",
    "        smoking = read_csv(DataFramesCSV.SMOKING_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Smoking Dataset',\n",
    "            name_file='du_smoking_dataset',\n",
    "            df_spark=smoking,\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=smoking,\n",
    "            dataset_name='Smoking',\n",
    "            name_file='du_smoking_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        smoking.drop(inplace=True, columns=[\"Code\"])\n",
    "        smoking.rename(\n",
    "            inplace=True,\n",
    "            columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.SMOKING_CSV)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=smoking,\n",
    "            metric_name_plot='Percentage Prevalence Tobacco use Adults',\n",
    "            metric_label='prevalence_smoking',\n",
    "            dataset_name='smoking',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='smoking', target_dataset=smoking)\n",
    "        \"\"\"\n",
    "\n",
    "    def _du_02_alcohol_consumption(self):\n",
    "        alcohol_consumption = read_csv(DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Alcohol Consumption Dataset',\n",
    "            name_file='du_alcohol_consumption_dataset',\n",
    "            df_spark=alcohol_consumption,\n",
    "            figure_size_height=1.2\n",
    "        )\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=alcohol_consumption,\n",
    "            dataset_name='Alcohol Consumption',\n",
    "            name_file='du_alcohol_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        alcohol_consumption.drop(inplace=True, columns=[\"Code\"])\n",
    "\n",
    "        alcohol_consumption.rename(\n",
    "            inplace=True,\n",
    "            columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=alcohol_consumption,\n",
    "            metric_name_plot='Liters of Pure Alcohol per Capita',\n",
    "            metric_label='liters_of_pure_alcohol_per_capita',\n",
    "            dataset_name='alcohol',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='alcohol_consumption', target_dataset=alcohol_consumption)\n",
    "        \"\"\"\n",
    "    \n",
    "    def _du_02_obesity(self):\n",
    "        obesity_dataset = read_csv(DataFramesCSV.WHO_OBESITY_CSV)\n",
    "        \"\"\"\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            dataset=obesity_dataset,\n",
    "            dataset_name='obesity',\n",
    "            name_file='du_obesity_summary'\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Obesity Dataset',\n",
    "            name_file='du_obesity_dataset',\n",
    "            df_spark=obesity_dataset,\n",
    "            figure_size_height=3.3\n",
    "        )\n",
    "        \"\"\"\n",
    "        # Sex if filtered to match both\n",
    "        obesity_dataset = obesity_dataset[obesity_dataset.Sex == \"Both sexes\"]\n",
    "        obesity_dataset = obesity_dataset[[\"Numeric\", \"Countries, territories and areas\", \"WHO region\", 'Year']]\n",
    "        obesity_dataset.rename(\n",
    "            inplace=True,\n",
    "            columns=pj.COLUMN_RENAME_BY_DATASET.get(pj.DataFramesCSV.WHO_OBESITY_CSV)\n",
    "        )\n",
    "\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=obesity_dataset,\n",
    "            metric_name_plot='Percentage Obesity',\n",
    "            metric_label='percentage_obesity',\n",
    "            dataset_name='obesity',\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "    def _du_02_happiness(self):\n",
    "        happiness_record = spark.read.format(\n",
    "            \"com.crealytics.spark.excel\"\n",
    "        ).option(\n",
    "            \"useHeader\", \"true\"\n",
    "        ).option(\n",
    "            \"inferSchema\", \"true\"\n",
    "        ).option(\n",
    "            \"dataAddress\", \"'Sheet1'!A1\"\n",
    "        ).load(\n",
    "            DataFramesXLSX.HAPPINESS_REPORT_XLSX.value\n",
    "        )\n",
    "        \n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Happiness Report Dataset',\n",
    "            name_file='du_happiness_dataset',\n",
    "            df_spark=pd.DataFrame(happiness_record),\n",
    "            figure_size_height=2.5\n",
    "        )\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=happiness_record,\n",
    "            dataset_name='Happiness',\n",
    "            name_file='du_happiness_summary',\n",
    "            figure_size_height=2.3\n",
    "        )\n",
    "        new_columns = [col.split(',', 2)[-1].strip() for col in happiness_record.columns]\n",
    "        happiness_record.columns = new_columns\n",
    "\n",
    "        happiness_record.rename(\n",
    "            inplace=True,\n",
    "            columns=COLUMN_RENAME_BY_DATASET.get(DataFramesXLSX.HAPPINESS_REPORT_XLSX)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self._merge_by_country_name(dataset_name='happiness', target_dataset=happiness_record)\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Life Ladder',\n",
    "            metric_label='life_ladder',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Social Support',\n",
    "            metric_label='social_support',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Freedom to Make Life Choices',\n",
    "            metric_label='freedom_to_make_life_choices',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Generosity',\n",
    "            metric_label='generosity',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Perceptions of Corruption',\n",
    "            metric_label='perceptions_of_corruption',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Positive Affect',\n",
    "            metric_label='positive_affect',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            dataset=happiness_record,\n",
    "            metric_name_plot='Negative affect',\n",
    "            metric_label='negative_affect',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        \"\"\"\n",
    "    def _function_mapper_du_02(self, dataset: Union[DataFramesCSV, DataFramesXLSX]):\n",
    "        mapper = {\n",
    "            DataFramesCSV.MEAT_CONSUMPTION_CSV: self._du_02_meat_consumption,\n",
    "            DataFramesCSV.WHO_OBESITY_CSV: self._du_02_obesity,\n",
    "            DataFramesXLSX.HAPPINESS_REPORT_XLSX: self._du_02_happiness,\n",
    "            # DataFramesCSV.HUNGER_CSV: self._du_02_hunger,\n",
    "            # DataFramesCSV.SMOKING_CSV: self._du_02_smoking,\n",
    "            # DataFramesCSV.ALCOHOL_CONSUMPTION_CSV: self._du_02_alcohol_consumption,\n",
    "        }\n",
    "\n",
    "        return mapper.get(dataset)\n",
    "    \n",
    "    def _du_02_run_processes(self):\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.MEAT_CONSUMPTION_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.WHO_OBESITY_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesXLSX.HAPPINESS_REPORT_XLSX)()\n",
    "        \"\"\"\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.HUNGER_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.SMOKING_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)()\n",
    "        \"\"\"\n",
    "\n",
    "    def du_02(self):\n",
    "        # pd.set_option('display.max_columns', None)\n",
    "        # pd.set_option('display.max_rows', None)\n",
    "        # pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "        self._du_02_country_master()\n",
    "        self._du_02_run_processes()\n",
    "\n",
    "        # self.get_crosstab_missing_countries(save_table=True).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = ProjectManager(\n",
    "    generate_images_du_02=True\n",
    ")\n",
    "# manager.country_master.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1745.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: com.crealytics.spark.excel. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [162]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmanager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdu_02\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [160]\u001B[0m, in \u001B[0;36mProjectManager.du_02\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdu_02\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;66;03m# pd.set_option('display.max_columns', None)\u001B[39;00m\n\u001B[1;32m    341\u001B[0m     \u001B[38;5;66;03m# pd.set_option('display.max_rows', None)\u001B[39;00m\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;66;03m# pd.set_option('display.expand_frame_repr', False)\u001B[39;00m\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_du_02_country_master()\n\u001B[0;32m--> 345\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_du_02_run_processes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [160]\u001B[0m, in \u001B[0;36mProjectManager._du_02_run_processes\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_mapper_du_02(dataset\u001B[38;5;241m=\u001B[39mDataFramesCSV\u001B[38;5;241m.\u001B[39mMEAT_CONSUMPTION_CSV)()\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_mapper_du_02(dataset\u001B[38;5;241m=\u001B[39mDataFramesCSV\u001B[38;5;241m.\u001B[39mWHO_OBESITY_CSV)()\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_function_mapper_du_02\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDataFramesXLSX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHAPPINESS_REPORT_XLSX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03mself._function_mapper_du_02(dataset=DataFramesCSV.HUNGER_CSV)()\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;124;03mself._function_mapper_du_02(dataset=DataFramesCSV.SMOKING_CSV)()\u001B[39;00m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03mself._function_mapper_du_02(dataset=DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)()\u001B[39;00m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "Input \u001B[0;32mIn [160]\u001B[0m, in \u001B[0;36mProjectManager._du_02_happiness\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_du_02_happiness\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    234\u001B[0m     happiness_record \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.crealytics.spark.excel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43museHeader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minferSchema\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    240\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdataAddress\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSheet1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m!A1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m--> 242\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mDataFramesXLSX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHAPPINESS_REPORT_XLSX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_capture_get_dataframe_info_image(\n\u001B[1;32m    247\u001B[0m         table_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHappiness Report Dataset\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    248\u001B[0m         name_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdu_happiness_dataset\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    249\u001B[0m         df_spark\u001B[38;5;241m=\u001B[39mpd\u001B[38;5;241m.\u001B[39mDataFrame(happiness_record),\n\u001B[1;32m    250\u001B[0m         figure_size_height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2.5\u001B[39m\n\u001B[1;32m    251\u001B[0m     )\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_capture_summary_dataset_to_image(\n\u001B[1;32m    253\u001B[0m         df_spark\u001B[38;5;241m=\u001B[39mhappiness_record,\n\u001B[1;32m    254\u001B[0m         dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHappiness\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    255\u001B[0m         name_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdu_happiness_summary\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    256\u001B[0m         figure_size_height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2.3\u001B[39m\n\u001B[1;32m    257\u001B[0m     )\n",
      "File \u001B[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py:158\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
      "File \u001B[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1745.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: com.crealytics.spark.excel. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "manager.du_02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columns': ['name', 'alpha-2', 'alpha-3', 'country-code', 'iso_3166-2', 'region', 'sub-region', 'intermediate-region', 'region-code', 'sub-region-code', 'intermediate-region-code'], 'dtypes': ['string', 'string', 'string', 'int', 'string', 'string', 'string', 'string', 'int', 'int', 'int'], 'count': [249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249], 'isnull': Row(name=0, alpha-2=0, alpha-3=0, country-code=0, iso_3166-2=0, region=1, sub-region=1, intermediate-region=142, region-code=1, sub-region-code=1, intermediate-region-code=142)}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "# Get column names truncated to 30 characters\n",
    "column_names = [col[:30] for col in df_spark.columns]\n",
    "\n",
    "# Get data types of columns\n",
    "column_types = [dtype for _, dtype in df_spark.dtypes]\n",
    "\n",
    "# Get count for each column\n",
    "row_count = df_spark.count()\n",
    "column_counts = [row_count for _ in df_spark.columns]\n",
    "\n",
    "# Get null count for each column\n",
    "column_null_counts = df_spark.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_spark.columns]).collect()[0]\n",
    "\n",
    "info_object = {\n",
    "    'columns': column_names,\n",
    "    'dtypes': column_types,\n",
    "    'count': column_counts,\n",
    "    'isnull': column_null_counts\n",
    "}\n",
    "\n",
    "print(info_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
