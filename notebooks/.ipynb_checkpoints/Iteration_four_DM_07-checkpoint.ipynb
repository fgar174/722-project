{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c18f2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F, window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "spark = SparkSession.builder.appName('Iteration4_DM').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.optimizer.maxIterations\", 300)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import enum\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark import ml\n",
    "\n",
    "PATH_MODELS = '../models/'\n",
    "PATH_IMAGES = '../tex/iterations/iteration_4/images/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bd201e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = spark.read.csv(\"../datasets/GENERATED/DP_03_M6.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2fb070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_confusion_matrix_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        dataframe: pd.DataFrame,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "        font_size=4,\n",
    "        head=None,\n",
    "        accuracy: float = None,\n",
    "        precision: float = None,\n",
    "        recall: float = None,\n",
    "        f1_score: float = None,\n",
    "        best_params=None,\n",
    "        best_training_accuracy=None,\n",
    "        best_test_accuracy=None,\n",
    "        tuning_scoring='precision',\n",
    "):\n",
    "    fig, (ax, ax2) = plt.subplots(figsize=(figure_size_width, figure_size_height), nrows=2)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    df_cm = ax.table(\n",
    "        cellText=dataframe.head(head).values if head is not None else dataframe.values,\n",
    "        colLabels=[' '.join(col.split('_')) for col in dataframe.columns],\n",
    "        colWidths=[0.2, 0.25, 0.25],\n",
    "        loc='center'\n",
    "    )\n",
    "    ax.set_title(table_name)\n",
    "    df_cm.auto_set_font_size(False)\n",
    "    df_cm.set_fontsize(font_size)\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.axis('tight')\n",
    "    if best_params:\n",
    "        df_best_parameters = pd.DataFrame(list(best_params.items()), columns=['parameter', 'best'])\n",
    "        data_table_best_params = ax2.table(\n",
    "            cellText=df_best_parameters.head(head).values if head is not None else df_best_parameters.values,\n",
    "            colLabels=[' '.join(col.split('_')) for col in df_best_parameters.columns],\n",
    "            colWidths=[0.5, 0.3],\n",
    "            loc='center',\n",
    "        )\n",
    "        ax2.set_title(f\"Tuning Process for {tuning_scoring}:\")\n",
    "        data_table_best_params.auto_set_font_size(False)\n",
    "        data_table_best_params.set_fontsize(font_size)\n",
    "\n",
    "    increase_lines = - 0.13\n",
    "    current_line = -0.2\n",
    "    font_size_text = font_size + 1\n",
    "    text_h_pos_m_r = 0.4\n",
    "    text_h_pos_m_t = 1\n",
    "\n",
    "    current_line = current_line + increase_lines\n",
    "\n",
    "    if accuracy is not None:\n",
    "        plt.text(\n",
    "            text_h_pos_m_r, current_line, f'Accuracy: {round(accuracy * 100, 2)}%',\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=font_size_text,\n",
    "        )\n",
    "    current_line = current_line + increase_lines\n",
    "    if precision is not None:\n",
    "        plt.text(\n",
    "            text_h_pos_m_r, current_line, f'Precision: {round(precision * 100, 2)}%',\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=font_size_text,\n",
    "        )\n",
    "\n",
    "    current_line = current_line + increase_lines\n",
    "    if recall is not None:\n",
    "        plt.text(\n",
    "            text_h_pos_m_r, current_line, f'Recall: {round(recall * 100, 2)}%',\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=font_size_text,\n",
    "        )\n",
    "    current_line = current_line + increase_lines\n",
    "    if f1_score is not None:\n",
    "        plt.text(\n",
    "            text_h_pos_m_r, current_line, f'F1-score: {round(f1_score * 100, 2)}%',\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=font_size_text,\n",
    "        )\n",
    "\n",
    "    # fig.tight_layout()\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def capture_table_dataframe_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        col_widths: List[float],\n",
    "        dataframe: pd.DataFrame,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "        font_size=4,\n",
    "        head=None,\n",
    "        show_records=True,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    data_table = ax.table(\n",
    "        cellText=dataframe.head(head).values if head is not None else dataframe.values,\n",
    "        colLabels=[' '.join(col.split('_')) for col in dataframe.columns],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "    num_rows = dataframe.shape[0] - 1\n",
    "    records = f'(Records: {num_rows})' if show_records else ''\n",
    "    plt.title(f'{table_name} {records}')\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(font_size)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "311cfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvailableModels(enum.Enum):\n",
    "    RANDOM_FOREST = 'Random Forest'\n",
    "    DECISION_TREE = 'Decision Tree'\n",
    "    LOGISTIC_REGRESSION = 'Logistic Regression'\n",
    "    GRADIENT_BOOSTING = 'Gradient Boosting'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelFitter:\n",
    "    dataset: pyspark.sql.dataframe.DataFrame\n",
    "    models_to_fit: List[AvailableModels]\n",
    "    models: Dict[AvailableModels, ml.Estimator] = None\n",
    "    predictions_models: Dict[AvailableModels, pd.Series] = None\n",
    "    tuning_scoring: str = 'accuracy'\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.load_models()\n",
    "        self.acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"expected_obesity_rate\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        self.rcl_evaluator = MulticlassClassificationEvaluator(labelCol=\"expected_obesity_rate\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "        self.prc_evaluator = MulticlassClassificationEvaluator(labelCol=\"expected_obesity_rate\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "    \n",
    "    def get_classifier(self, enum_model: AvailableModels):\n",
    "        return self.models[enum_model]\n",
    "    \n",
    "    @property\n",
    "    def random_forest(self):\n",
    "        return self.models[AvailableModels.RANDOM_FOREST]\n",
    "    \n",
    "    @property\n",
    "    def decision_tree(self):\n",
    "        return self.models[AvailableModels.DECISION_TREE]\n",
    "    \n",
    "    @property\n",
    "    def logistic_regresion(self):\n",
    "        return self.models[AvailableModels.LOGISTIC_REGRESSION]\n",
    "    \n",
    "    @property\n",
    "    def gradient_boosting(self):\n",
    "        return self.models[AvailableModels.GRADIENT_BOOSTING]\n",
    "    \n",
    "        \n",
    "    @property\n",
    "    def random_forest_predictions(self):\n",
    "        return self.predictions_models.get(AvailableModels.RANDOM_FOREST, None)\n",
    "    \n",
    "    @property\n",
    "    def decision_tree_predictions(self):\n",
    "        return self.predictions_models.get(AvailableModels.DECISION_TREE, None)\n",
    "    \n",
    "    @property\n",
    "    def logistic_regresion_predictions(self):\n",
    "        return self.predictions_models.get(AvailableModels.LOGISTIC_REGRESSION, None)\n",
    "    \n",
    "    @property\n",
    "    def gradient_boosting_predictions(self):\n",
    "        return self.predictions_models.get(AvailableModels.GRADIENT_BOOSTING, None)\n",
    "\n",
    "    def load_models(self):\n",
    "        self.predictions_models = {}\n",
    "        for path_enum in AvailableModels:\n",
    "            path_file = f\"{PATH_MODELS}/{'_'.join(path_enum.value.lower())}\"\n",
    "            if os.path.exists(path_file):\n",
    "                if path_enum == AvailableModels.RANDOM_FOREST:\n",
    "                    self.models[path_enum] = ml.classification.RandomForestClassificationModel.load(path_file)\n",
    "                if path_enum == AvailableModels.LOGISTIC_REGRESSION:\n",
    "                    self.models[path_enum] = ml.classification.LogisticRegression.load(path_file)\n",
    "                if path_enum == AvailableModels.GRADIENT_BOOSTING:\n",
    "                    self.models[path_enum] = ml.classification.GBTClassificationModel.load(path_file)\n",
    "                if path_enum == AvailableModels.DECISION_TREE:\n",
    "                    self.models[path_enum] = ml.classification.DecisionTreeClassificationModel.load(path_file)\n",
    "\n",
    "    def save_model(self, classifiers: ml, path_enum: AvailableModels):\n",
    "        if self.models.get(path_enum, None) is None:\n",
    "            path_file = f\"{PATH_MODELS}/{'_'.join(path_enum.value.lower())}\"\n",
    "            classifiers.save(path_file)\n",
    "\n",
    "    def _get_features_columns_names(self):\n",
    "        return [\n",
    "            \"prevalence_smoking\",\n",
    "            \"liters_of_pure_alcohol_per_capita\",\n",
    "            \"beef\",\n",
    "            \"poultry\",\n",
    "            \"sheep_and_goat\",\n",
    "            \"pig\",\n",
    "            \"fish_and_seafood\",\n",
    "            \"perceptions_of_corruption\",\n",
    "            \"negative_affect\",\n",
    "            \"life_ladder\",\n",
    "            \"positive_affect\",\n",
    "            \"freedom_to_make_life_choices\",\n",
    "            \"generosity\",\n",
    "            \"social_support\",\n",
    "        ]\n",
    "    \n",
    "    def _generate_split_dataset(self):\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=self._get_features_columns_names(),\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        output = assembler.transform(self.dataset)\n",
    "        final_data = output.select(\"features\", \"expected_obesity_rate\")\n",
    "        train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "        \n",
    "        return train_data, test_data, assembler\n",
    "\n",
    "    def fit_models(self) -> None:\n",
    "        \"\"\"Fit specified models to the data.\"\"\"\n",
    "        fit_function_mapper = {\n",
    "            AvailableModels.LOGISTIC_REGRESSION: self._fit_logistic_regression,\n",
    "            AvailableModels.DECISION_TREE: self._fit_decision_tree,\n",
    "            AvailableModels.RANDOM_FOREST: self._fit_random_forest,\n",
    "            AvailableModels.GRADIENT_BOOSTING: self._fit_gradient_boosting,\n",
    "        }\n",
    "\n",
    "        for model_enum in self.models_to_fit:\n",
    "            function_to_run = fit_function_mapper.get(model_enum)\n",
    "            if function_to_run:\n",
    "                print(f\"Fitting : {model_enum.value}...\")\n",
    "                function_to_run()\n",
    "\n",
    "            # If you want to calculate accuracy, ensure your fitting functions return predictions\n",
    "            # and uncomment the below code:\n",
    "            # predictions = function_to_run()\n",
    "            # accuracy = np.mean(predictions == self._y_test)\n",
    "            # print(f\"Accuracy {model_enum.value}: {accuracy}\")\n",
    "\n",
    "    def generate_confusion_matrix_spark(self, enum_model):\n",
    "        predictions = self.predictions_models[enum_model]\n",
    "\n",
    "        # Computing TP, TN, FP, FN\n",
    "        TP = predictions.where((F.col('expected_obesity_rate') == 1) & (F.col('prediction') == 1)).count()\n",
    "        TN = predictions.where((F.col('expected_obesity_rate') == 0) & (F.col('prediction') == 0)).count()\n",
    "        FP = predictions.where((F.col('expected_obesity_rate') == 0) & (F.col('prediction') == 1)).count()\n",
    "        FN = predictions.where((F.col('expected_obesity_rate') == 1) & (F.col('prediction') == 0)).count()\n",
    "\n",
    "        confusion_matrix = [[TN, FP], [FN, TP]]\n",
    "\n",
    "        # Computing other metrics\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"expected_obesity_rate\", predictionCol=\"prediction\")\n",
    "\n",
    "        accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "        recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "        precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "        f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "        # Convert confusion matrix to a DataFrame for display purposes\n",
    "        df_cm = spark.createDataFrame(confusion_matrix, [\"Actual: 0\", \"Actual: 1\"])\n",
    "\n",
    "        # This is where you would call your capture_confusion_matrix_image function \n",
    "        # or adapt it to work with Spark DataFrames.\n",
    "        # For the sake of brevity, I'll not include that part here.\n",
    "\n",
    "        capture_confusion_matrix_image(\n",
    "            table_name=f'Confusion Matrix: {enum_model.value}',\n",
    "            dataframe=df_cm.toPandas(),\n",
    "            font_size=6,\n",
    "            name_file=f\"dm_confu_mat_{'_'.join([word.lower()[0:4] for word in enum_model.value.split(' ')])}\",\n",
    "            figure_size_width=4,\n",
    "            figure_size_height=3,\n",
    "            accuracy=accuracy,\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1_score=f1,\n",
    "            #best_params=best_params,\n",
    "            #best_training_accuracy=best_training_accuracy,\n",
    "            #best_test_accuracy=best_test_accuracy,\n",
    "        )\n",
    "    \n",
    "    def get_metrics(self, model_enum: AvailableModels):\n",
    "        predictions = self.predictions_models[model_enum]\n",
    "        return {\n",
    "            \"accuracy\": self.acc_evaluator.evaluate(predictions),\n",
    "            \"recall\": self.rcl_evaluator.evaluate(predictions),\n",
    "            \"precision\":self.prc_evaluator.evaluate(predictions),\n",
    "        }\n",
    "    \n",
    "    def _predictor_resume_logistic_regression(self, feature_importance):\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_importance.keys(),\n",
    "            'Importance': feature_importance.values()\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.round(5)\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        capture_table_dataframe_image(\n",
    "            dataframe=feature_importance_df,\n",
    "            table_name='Feature Importance: Logistic Regresion',\n",
    "            font_size=5,\n",
    "            name_file='dm_featu_imp_logi_regr',\n",
    "            col_widths=[0.2, 1.2, 0.6],\n",
    "            figure_size_height=3.5,\n",
    "            figure_size_width=8,\n",
    "            show_records=False\n",
    "        )\n",
    "\n",
    "    def _predictor_resume_gradient_boosting(self, feature_importance):\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_importance.keys(),\n",
    "            'Importance': feature_importance.values()\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.round(5)\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        capture_table_dataframe_image(\n",
    "            dataframe=feature_importance_df,\n",
    "            table_name='Feature Importance: Gradient Boosting',\n",
    "            font_size=5,\n",
    "            name_file='dm_featu_imp_grad_boost',\n",
    "            col_widths=[0.2, 1.2, 0.6],\n",
    "            figure_size_height=3.5,\n",
    "            figure_size_width=8,\n",
    "            show_records=False\n",
    "        )\n",
    "\n",
    "    def _predictor_resume_decision_tree(self, feature_importance):\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_importance.keys(),\n",
    "            'Importance': feature_importance.values()\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.round(5)\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        capture_table_dataframe_image(\n",
    "            dataframe=feature_importance_df,\n",
    "            table_name='Feature Importance: Decision Tree',\n",
    "            font_size=5,\n",
    "            name_file='dm_featu_imp_deci_tree',\n",
    "            col_widths=[0.2, 1.2, 0.6],\n",
    "            figure_size_height=3.5,\n",
    "            figure_size_width=8,\n",
    "            show_records=False\n",
    "        )\n",
    "        plt.savefig(f\"{PATH_IMAGES}decision_tree_tree.png\", dpi=500, bbox_inches='tight')\n",
    "\n",
    "    def _predictor_resume_random_forest(self, feature_importance):\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_importance.keys(),\n",
    "            'Importance': feature_importance.values()\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.round(5)\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        capture_table_dataframe_image(\n",
    "            dataframe=feature_importance_df,\n",
    "            table_name='Feature Importance: Random Forest',\n",
    "            font_size=5,\n",
    "            name_file='dm_featu_imp_random_forest',\n",
    "            col_widths=[0.2, 1.2, 0.6],\n",
    "            figure_size_height=3.5,\n",
    "            figure_size_width=8,\n",
    "            show_records=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def _fit_decision_tree(self) -> None:\n",
    "        train_data, test_data, assembler = self._generate_split_dataset()\n",
    "        dtc = DecisionTreeClassifier(labelCol='expected_obesity_rate',featuresCol='features')\n",
    "        dtc_model = dtc.fit(train_data)\n",
    "        self.predictions_models[AvailableModels.DECISION_TREE] = dtc_model.transform(test_data)\n",
    "        \n",
    "        \n",
    "        importances = dtc_model.featureImportances.toArray()\n",
    "        feature_names = assembler.getInputCols()\n",
    "        features_and_importances = list(zip(feature_names, importances))\n",
    "        features_and_importances_sorted = sorted(features_and_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        feature_importance = {feature:round(importance,4) for feature, importance in features_and_importances_sorted}\n",
    "\n",
    "        print(feature_importance)\n",
    "        \n",
    "        self.generate_confusion_matrix_spark(\n",
    "            enum_model=AvailableModels.DECISION_TREE\n",
    "        )\n",
    "        self._predictor_resume_decision_tree(feature_importance)\n",
    "\n",
    "    def _fit_logistic_regression(self) -> None:\n",
    "        print(\"FIT LOGISTIC REGRESION\")\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=self._get_features_columns_names(),\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        train_data_lg, test_data_lg = self.dataset.na.drop().randomSplit([0.7,.3])\n",
    "        log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='expected_obesity_rate')\n",
    "        pipeline = Pipeline(stages=[assembler, log_reg_titanic])\n",
    "        fit_model = pipeline.fit(train_data_lg)\n",
    "        \n",
    "        self.predictions_models[AvailableModels.LOGISTIC_REGRESSION] = fit_model.transform(test_data_lg)\n",
    "        \n",
    "        lr_model = fit_model.stages[-1]\n",
    "        importances = lr_model.coefficients\n",
    "        feature_importance = {\n",
    "            k:v for k, v in\n",
    "            list(zip(self._get_features_columns_names(), importances))\n",
    "        }\n",
    "        \n",
    "        self._predictor_resume_logistic_regression(feature_importance)\n",
    "        \n",
    "        self.generate_confusion_matrix_spark(\n",
    "            enum_model=AvailableModels.LOGISTIC_REGRESSION\n",
    "        )\n",
    "\n",
    "    def _fit_gradient_boosting(self) -> None:\n",
    "        print(\"FIT GRADIENT BOOSTING\")\n",
    "        train_data, test_data, assembler = self._generate_split_dataset()\n",
    "        gbt = GBTClassifier(labelCol='expected_obesity_rate',featuresCol='features')\n",
    "        gbt_model = gbt.fit(train_data)\n",
    "        self.predictions_models[AvailableModels.GRADIENT_BOOSTING] = gbt_model.transform(test_data)\n",
    "        \n",
    "        importances = gbt_model.featureImportances.toArray()\n",
    "        feature_names = assembler.getInputCols()\n",
    "        features_and_importances = list(zip(feature_names, importances))\n",
    "        features_and_importances_sorted = sorted(features_and_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        feature_importance = {feature:round(importance,4) for feature, importance in features_and_importances_sorted}\n",
    "        \n",
    "        self.generate_confusion_matrix_spark(\n",
    "            enum_model=AvailableModels.GRADIENT_BOOSTING\n",
    "        )\n",
    "        self._predictor_resume_gradient_boosting(feature_importance)\n",
    "\n",
    "    def _fit_random_forest(self) -> None:\n",
    "        print(\"FIT RANDOM FOREST\")\n",
    "        train_data, test_data, assembler = self._generate_split_dataset()\n",
    "        rfc = RandomForestClassifier(labelCol='expected_obesity_rate',featuresCol='features')\n",
    "        rfc_model = rfc.fit(train_data)\n",
    "        self.predictions_models[AvailableModels.RANDOM_FOREST] = rfc_model.transform(test_data)\n",
    "        \n",
    "        \n",
    "        importances = rfc_model.featureImportances.toArray()\n",
    "        feature_names = assembler.getInputCols()\n",
    "        features_and_importances = list(zip(feature_names, importances))\n",
    "        features_and_importances_sorted = sorted(features_and_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        feature_importance = {feature:round(importance,4) for feature, importance in features_and_importances_sorted}\n",
    "\n",
    "        print(feature_importance)\n",
    "        \n",
    "        self._predictor_resume_random_forest(feature_importance)\n",
    "        \n",
    "        self.generate_confusion_matrix_spark(\n",
    "            enum_model=AvailableModels.RANDOM_FOREST\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c43fa2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting : Decision Tree...\n",
      "{'pig': 0.3145, 'negative_affect': 0.1197, 'liters_of_pure_alcohol_per_capita': 0.1146, 'beef': 0.1015, 'fish_and_seafood': 0.0944, 'sheep_and_goat': 0.0751, 'poultry': 0.0651, 'social_support': 0.0622, 'prevalence_smoking': 0.0528, 'perceptions_of_corruption': 0.0, 'life_ladder': 0.0, 'positive_affect': 0.0, 'freedom_to_make_life_choices': 0.0, 'generosity': 0.0}\n",
      "Fitting : Random Forest...\n",
      "FIT RANDOM FOREST\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [174]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fitter \u001b[38;5;241m=\u001b[39m ModelFitter(\n\u001b[1;32m      2\u001b[0m     models_to_fit\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m         AvailableModels\u001b[38;5;241m.\u001b[39mDECISION_TREE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mDATASET,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mfitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36mModelFitter.fit_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function_to_run:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_enum\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mfunction_to_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36mModelFitter._fit_random_forest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m rfc_model \u001b[38;5;241m=\u001b[39m rfc\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions_models[AvailableModels\u001b[38;5;241m.\u001b[39mRANDOM_FOREST] \u001b[38;5;241m=\u001b[39m rfc_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAvailableModels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRANDOM_FOREST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m importances \u001b[38;5;241m=\u001b[39m rfc_model\u001b[38;5;241m.\u001b[39mfeatureImportances\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[1;32m    332\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m assembler\u001b[38;5;241m.\u001b[39mgetInputCols()\n",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36mModelFitter.save_model\u001b[0;34m(self, classifiers, path_enum)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, classifiers: ml, path_enum: AvailableModels):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(path_enum, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         path_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATH_MODELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path_enum\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         classifiers\u001b[38;5;241m.\u001b[39msave(path_file)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitter = ModelFitter(\n",
    "    models_to_fit=[\n",
    "        AvailableModels.DECISION_TREE,\n",
    "        AvailableModels.RANDOM_FOREST,\n",
    "        AvailableModels.GRADIENT_BOOSTING,\n",
    "        AvailableModels.LOGISTIC_REGRESSION,\n",
    "    ],\n",
    "    dataset=DATASET,\n",
    ")\n",
    "fitter.fit_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e2f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621966e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
