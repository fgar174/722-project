{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "spark = SparkSession.builder.appName('Iteration4').getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "import enum\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "databases_path = '../datasets/'\n",
    "PATH_IMAGES = '../tex/iterations/iteration_4/images/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Some methods and enums that will be used for:\n",
    "\n",
    "- Produce process images\n",
    "- Functions to execute repetitive tasks\n",
    "\n",
    "This will make easier the workflow of the project and also the artifact generation automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFramesCSV(enum.Enum):\n",
    "    ALCOHOL_CONSUMPTION_CSV = f\"{databases_path}4_total-alcohol-consumption-per-capita-litres-of-pure-alcohol.csv\"\n",
    "    COUNTRY_MASTER_CSV = f\"{databases_path}0_master_country_codes.csv\"\n",
    "    WHO_OBESITY_CSV = f\"{databases_path}1_who_obesity.csv\"\n",
    "    MEAT_CONSUMPTION_CSV = f\"{databases_path}2_meat_consumption.csv\"\n",
    "    HUNGER_CSV = f\"{databases_path}5_global_hunger_index.csv\"\n",
    "    SMOKING_CSV = f\"{databases_path}6_share-of-adults-who-smoke.csv\"\n",
    "    HAPPINESS_REPORT_CSV = f\"{databases_path}3_happiness_report.csv\"\n",
    "\n",
    "class DataFramePreviousFieldNameOptions(enum.Enum):\n",
    "    IS_NULL = 'isnull'\n",
    "    D_TYPES = 'dtypes'\n",
    "    COUNT = 'count'\n",
    "\n",
    "COLUMN_RENAME_BY_DATASET = {\n",
    "    DataFramesCSV.WHO_OBESITY_CSV: {\n",
    "        'Numeric': 'percentage_obesity',\n",
    "        'Countries, territories and areas': 'country',\n",
    "        'WHO region': 'region',\n",
    "        'Year': 'year',\n",
    "    },\n",
    "    DataFramesCSV.HAPPINESS_REPORT_CSV: {\n",
    "        'year': 'year',\n",
    "        'Country name': 'country',\n",
    "        \"Life Ladder\": 'life_ladder',\n",
    "        \"Social support\": 'social_support',\n",
    "        \"Freedom to make life choices\": \"freedom_to_make_life_choices\",\n",
    "        \"Generosity\": \"generosity\",\n",
    "        \"Perceptions of corruption\": \"perceptions_of_corruption\",\n",
    "        \"Positive affect\": \"positive_affect\",\n",
    "        \"Negative affect\": \"negative_affect\",\n",
    "    },\n",
    "    DataFramesCSV.MEAT_CONSUMPTION_CSV: {\n",
    "        'Code': 'country_code',\n",
    "        'Year': 'year',\n",
    "        \"Meat, poultry | 00002734 || Food available for consumption | 0645pc || kilograms per year per capita\": \"poultry\",\n",
    "        \"Meat, beef | 00002731 || Food available for consumption | 0645pc || kilograms per year per capita\": \"beef\",\n",
    "        \"Meat, sheep and goat | 00002732 || Food available for consumption | 0645pc || kilograms per year per capita\": \"sheep_and_goat\",\n",
    "        \"Meat, pig | 00002733 || Food available for consumption | 0645pc || kilograms per year per capita\": \"pig\",\n",
    "        \"Fish and seafood | 00002960 || Food available for consumption | 0645pc || kilograms per year per capita\": \"fish_and_seafood\",\n",
    "    },\n",
    "    DataFramesCSV.COUNTRY_MASTER_CSV: {\n",
    "        'alpha-3': 'country_code',\n",
    "        'name': 'country'\n",
    "    },\n",
    "    DataFramesCSV.HUNGER_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'Global Hunger Index (2021)': 'hunger_index',\n",
    "    },\n",
    "    DataFramesCSV.SMOKING_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'Prevalence of current tobacco use (% of adults)': 'prevalence_smoking',\n",
    "    },\n",
    "    DataFramesCSV.ALCOHOL_CONSUMPTION_CSV: {\n",
    "        'Entity': 'country',\n",
    "        'Year': 'year',\n",
    "        'liters_of_pure_alcohol_per_capita': 'liters_of_pure_alcohol_per_capita',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def capture_get_dataframe_info_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        previous_data: list = None,\n",
    "        previous_data_name: str = None,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "):\n",
    "\n",
    "    # Collect required statistics from PySpark DataFrame\n",
    "    column_names = [col[:30] for col in df_spark.columns]\n",
    "    column_types = [dtype for _, dtype in df_spark.dtypes]\n",
    "    row_count = df_spark.count()\n",
    "    column_counts = [row_count for _ in df_spark.columns]\n",
    "    column_null_counts = df_spark.agg(*[F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_spark.columns]).collect()[0]\n",
    "    column_non_null_counts = [row_count - null_count for null_count in column_null_counts]\n",
    "\n",
    "    info_object = {\n",
    "        'columns': column_names,\n",
    "        'dtypes': column_types,\n",
    "        'count': column_non_null_counts,\n",
    "        'isnull': column_null_counts\n",
    "    }\n",
    "\n",
    "    if previous_data is not None:\n",
    "        info_object[f'old {previous_data_name}'] = previous_data\n",
    "        current_data = info_object[previous_data_name]\n",
    "        info_object['change'] = [prev - curr for prev, curr in zip(previous_data, current_data)]\n",
    "\n",
    "    dataframe_info = list(zip(*info_object.values()))\n",
    "\n",
    "    # Plotting the table image\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    col_widths = [0.35, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "    data_table = ax.table(\n",
    "        cellText=dataframe_info,\n",
    "        colLabels=[' '.join(col.split('_')) for col in info_object.keys()],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    if previous_data is not None:\n",
    "        for (i, j), val in np.ndenumerate(np.array(dataframe_info)):\n",
    "            if j == 6 and val != 0:  # We look into the last column (j==6), and search for non-zero values\n",
    "                data_table[(i + 1, j)].set_facecolor(\"red\")\n",
    "                data_table[(i + 1, j)].set_text_props(color='white', weight='bold')\n",
    "\n",
    "    num_rows = row_count - 1\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(6)\n",
    "    plt.title(f'{table_name} (Records: {num_rows})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def capture_summary_dataset_to_image(\n",
    "        name_file: str,\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        dataset_name: str,\n",
    "        figure_size_height=3,\n",
    "        font_size=7,\n",
    "):\n",
    "    dataset = df_spark.toPandas()\n",
    "    desc = dataset.describe().round(2).T\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, figure_size_height))\n",
    "    new_order = ['min', '25%', '50%', '75%', 'max', 'mean', 'count', 'std']\n",
    "    desc = desc[new_order]\n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    col_widths = [0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08]\n",
    "\n",
    "    data_table = ax.table(\n",
    "        cellText=desc.values,\n",
    "        colLabels=desc.columns,\n",
    "        rowLabels=[name[:30] for name in desc.index],\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=col_widths,\n",
    "    )\n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(font_size)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.title(f\"Descriptive Statistics {dataset_name}\")\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def du_data_exploration_basics(\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        metric_name_plot: str,\n",
    "        metric_label: str,\n",
    "        dataset_name: str,\n",
    "        country_label: str = 'country',\n",
    "        prefix_file_name: str = 'du',\n",
    "        year_label: str = 'year',\n",
    "):\n",
    "\n",
    "    metric_name_file = '_'.join([word[0:2] for word in metric_label.split('_')])\n",
    "    base_name_file = f'{prefix_file_name}_{dataset_name}_{metric_name_file}'\n",
    "    base_name_file_with_path = f'{PATH_IMAGES}{base_name_file}'\n",
    "    \n",
    "    df_spark.toPandas().info()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(20, 10), nrows=2, ncols=1)\n",
    "    grouped_data = df_spark.groupBy(year_label).agg(F.collect_list(metric_label).alias(\"values\"))\n",
    "    data_collected = grouped_data.collect()\n",
    "\n",
    "    years = [row[year_label] for row in data_collected]\n",
    "    data_to_plot = [row['values'] for row in data_collected]\n",
    "    \n",
    "    ax1.boxplot(data_to_plot, vert=True, patch_artist=True, labels=years)\n",
    "    ax1.set_title(f'Yearly Spread of {metric_name_plot}')\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel(metric_name_plot)\n",
    "    \n",
    "\n",
    "    average_per_year = df_spark.groupBy(year_label).agg(F.avg(metric_label).alias('avg_metric'))\n",
    "    average_per_year = average_per_year.orderBy(year_label)\n",
    "    data_collected = average_per_year.collect()\n",
    "\n",
    "    years = [row[year_label] for row in data_collected]\n",
    "    avg_values = [row['avg_metric'] for row in data_collected]\n",
    "    \n",
    "    \n",
    "    ax2.plot(years, avg_values, linestyle='-', marker='o', color='b', label=f'Average {metric_name_plot}')\n",
    "    ax2.set_title(f'Average {metric_name_plot} Over Years')\n",
    "    ax2.set_ylabel(metric_name_plot)\n",
    "    ax2.set_xlabel('Year')\n",
    "    \n",
    "    plt.suptitle('')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{base_name_file_with_path}_y_trend.png', bbox_inches='tight', dpi=80)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    data_collected = df_spark.select(metric_label).rdd.flatMap(lambda x: x).collect()\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.hist(data_collected, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of {metric_name_plot}')\n",
    "    plt.xlabel(metric_name_plot)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(f'{base_name_file_with_path}_freq.png', bbox_inches='tight', dpi=120)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    average_by_country = df_spark.groupBy(country_label).agg(F.avg(metric_label).alias(\"avg_metric\"))\n",
    "    top_countries = average_by_country.orderBy(F.desc(\"avg_metric\")).limit(20)\n",
    "    bottom_countries = average_by_country.orderBy(\"avg_metric\").limit(20)\n",
    "\n",
    "    top_countries_list = [row[country_label] for row in top_countries.collect()]\n",
    "    bottom_countries_list = [row[country_label] for row in bottom_countries.collect()]\n",
    "\n",
    "    top_countries_data = df_spark.filter(df_spark[country_label].isin(top_countries_list))\n",
    "    bottom_countries_data = df_spark.filter(df_spark[country_label].isin(bottom_countries_list))\n",
    "\n",
    "    top_countries_pd = top_countries_data.toPandas()\n",
    "    bottom_countries_pd = bottom_countries_data.toPandas()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(18, 12))\n",
    "    fig.tight_layout(pad=10.0)\n",
    "\n",
    "    top_countries_pd.boxplot(column=metric_label, by=country_label, ax=ax1, rot=80)\n",
    "    ax1.set_title(f'{metric_name_plot} for Top 20 Countries')\n",
    "    ax1.set_xlabel('Country')\n",
    "    ax1.set_ylabel(metric_name_plot)\n",
    "\n",
    "    bottom_countries_pd.boxplot(column=metric_label, by=country_label, ax=ax2, rot=80)\n",
    "    ax2.set_title(f'{metric_name_plot} for Lowest 20 Countries')\n",
    "    ax2.set_xlabel('Country')\n",
    "    ax2.set_ylabel(metric_name_plot)\n",
    "\n",
    "    plt.suptitle('')\n",
    "    plt.savefig(f'{base_name_file_with_path}_cou_t_l_20.png', bbox_inches='tight', dpi=80)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Plotting aggregated data\n",
    "    merged_pd = pd.concat([top_countries_pd, bottom_countries_pd])\n",
    "    avg_values_by_country = merged_pd.groupby(country_label)[metric_label].mean().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    avg_values_by_country.plot(kind='barh', color='skyblue')\n",
    "    plt.title(f'Average {metric_name_plot} by Country 20 top and 20 lowest')\n",
    "    plt.xlabel(metric_name_plot)\n",
    "    plt.ylabel('Country')\n",
    "    plt.savefig(f'{base_name_file_with_path}_cou_t_l_20_v2.png', bbox_inches='tight', dpi=80)\n",
    "    plt.close()\n",
    "\n",
    "def capture_table_dataframe_image(\n",
    "        table_name: str,\n",
    "        name_file: str,\n",
    "        col_widths: List[float],\n",
    "        df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "        figure_size_height=10.0,\n",
    "        figure_size_width=5.0,\n",
    "        font_size=4,\n",
    "        head=None,\n",
    "        show_records=True,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(figure_size_width, figure_size_height))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    # Convert the Spark DataFrame to a list of lists and retrieve column names\n",
    "    if head:\n",
    "        data_values = df_spark.limit(head).collect()\n",
    "    else:\n",
    "        data_values = df_spark.collect()\n",
    "    columns = df_spark.columns\n",
    "    \n",
    "    # Create table with the collected data and columns\n",
    "    data_table = ax.table(\n",
    "        cellText=data_values,\n",
    "        colLabels=[' '.join(col.split('_')) for col in columns],\n",
    "        colWidths=col_widths,\n",
    "        loc='center'\n",
    "    )\n",
    "    \n",
    "    num_rows = df_spark.count() - 1\n",
    "    records = f'(Records: {num_rows})' if show_records else ''\n",
    "    plt.title(f'{table_name} {records}')\n",
    "    \n",
    "    data_table.auto_set_font_size(False)\n",
    "    data_table.set_fontsize(font_size)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{PATH_IMAGES}{name_file}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def rename_columns(\n",
    "    df: pyspark.sql.dataframe.DataFrame,\n",
    "    map_columns: Dict[str,str]\n",
    ") -> pyspark.sql.dataframe.DataFrame:\n",
    "    for old_name, new_name in map_columns.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    return df\n",
    "\n",
    "def read_csv(file_path_enum: DataFramesCSV, sep=\",\") -> pyspark.sql.dataframe.DataFrame:\n",
    "    return spark.read.csv(file_path_enum.value, header=True, inferSchema=True, sep=sep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crisp Process Manager\n",
    "\n",
    "I will create a manager that will make easier to follow each steps for the reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CRISPManager:\n",
    "    missing_countries = None\n",
    "    country_master = None\n",
    "    integrated_dataset = None\n",
    "    generate_images_du_02: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        country_master = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)\n",
    "        country_master = rename_columns(\n",
    "            df=country_master,\n",
    "            map_columns={\n",
    "                \"alpha-3\": \"country_code\",\n",
    "                \"name\": \"country\"\n",
    "            }\n",
    "        )\n",
    "        self.country_master = country_master\n",
    "\n",
    "    def _capture_get_dataframe_info_image(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            name_file: str,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            figure_size_height=10.0,\n",
    "            figure_size_width=5.0,\n",
    "            force_save_image=False,\n",
    "            previous_data: pd.Series = None,\n",
    "            previous_data_name: DataFramePreviousFieldNameOptions = None,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_get_dataframe_info_image(\n",
    "                table_name=table_name,\n",
    "                name_file=name_file,\n",
    "                df_spark=df_spark,\n",
    "                figure_size_height=figure_size_height,\n",
    "                figure_size_width=figure_size_width,\n",
    "                previous_data_name=previous_data_name,\n",
    "                previous_data=previous_data,\n",
    "            )\n",
    "    \n",
    "    def _capture_summary_dataset_to_image(\n",
    "            self,\n",
    "            name_file: str,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            dataset_name: str,\n",
    "            figure_size_height=3,\n",
    "            font_size=7,\n",
    "            force_save_image=False,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_summary_dataset_to_image(\n",
    "                name_file=name_file,\n",
    "                df_spark=df_spark,\n",
    "                dataset_name=dataset_name,\n",
    "                figure_size_height=figure_size_height,\n",
    "                font_size=font_size,\n",
    "            )\n",
    "\n",
    "    def _capture_table_dataframe_image(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            name_file: str,\n",
    "            col_widths: List[float],\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            figure_size_height=10.0,\n",
    "            figure_size_width=5.0,\n",
    "            font_size=4,\n",
    "            head=None,\n",
    "            force_save_image=False,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            capture_table_dataframe_image(\n",
    "                table_name=table_name,\n",
    "                name_file=name_file,\n",
    "                col_widths=col_widths,\n",
    "                df_spark=df_spark,\n",
    "                figure_size_width=figure_size_width,\n",
    "                figure_size_height=figure_size_height,\n",
    "                font_size=font_size,\n",
    "                head=head\n",
    "            )\n",
    "\n",
    "\n",
    "    def _du_data_exploration_basics(\n",
    "            self,\n",
    "            df_spark: pyspark.sql.dataframe.DataFrame,\n",
    "            metric_name_plot: str,\n",
    "            metric_label: str,\n",
    "            dataset_name: str,\n",
    "            country_label: str = 'country',\n",
    "            prefix_file_name: str = 'du',\n",
    "            year_label: str = 'year',\n",
    "            force_save_image=False,\n",
    "    ):\n",
    "        if self.generate_images_du_02 or force_save_image:\n",
    "            du_data_exploration_basics(\n",
    "                dataset_name=dataset_name,\n",
    "                df_spark=df_spark,\n",
    "                metric_label=metric_label,\n",
    "                metric_name_plot=metric_name_plot,\n",
    "                country_label=country_label,\n",
    "                prefix_file_name=prefix_file_name,\n",
    "                year_label=year_label,\n",
    "            )\n",
    "    \n",
    "    def _merge_by_country_code(self, dataset_name, target_dataset):\n",
    "        # 2. Full join on the \"id\" column\n",
    "        full_joined = target_dataset.join(self.country_master, \"country_code\", \"left\")\n",
    "        not_found = full_joined.filter(F.col(\"country-code\").isNull()).select(\"country_code\").distinct()\n",
    "        \n",
    "        not_found = not_found.withColumn(\n",
    "            \"dataset\", lit(dataset_name)\n",
    "        ).withColumn(\n",
    "            \"value\", lit(1)\n",
    "        ).withColumn(\n",
    "            \"replacement\", lit(None)\n",
    "        )\n",
    "        not_found = rename_columns(\n",
    "            df=not_found,\n",
    "            map_columns={'country_code':'missing'}\n",
    "        )\n",
    "        if self.missing_countries is None:\n",
    "            self.missing_countries = not_found\n",
    "        else:\n",
    "            self.missing_countries = not_found.union(self.missing_countries)\n",
    "\n",
    "    def _merge_by_country_name(self, dataset_name, target_dataset: pd.DataFrame):\n",
    "        # 2. Full join on the \"id\" column\n",
    "        full_joined = target_dataset.join(self.country_master, \"country\", \"left\")\n",
    "        not_found = full_joined.filter(F.col(\"country-code\").isNull()).select(\"country\").distinct()\n",
    "        \n",
    "        not_found = not_found.withColumn(\n",
    "            \"dataset\", lit(dataset_name)\n",
    "        ).withColumn(\n",
    "            \"value\", lit(1)\n",
    "        ).withColumn(\n",
    "            \"replacement\", lit(None)\n",
    "        )\n",
    "        not_found = rename_columns(\n",
    "            df=not_found,\n",
    "            map_columns={'country':'missing'}\n",
    "        )\n",
    "        if self.missing_countries is None:\n",
    "            self.missing_countries = not_found\n",
    "        else:\n",
    "            self.missing_countries = not_found.union(self.missing_countries)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _du_02_country_master(self):\n",
    "        country_master = read_csv(DataFramesCSV.COUNTRY_MASTER_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Countries Dataset',\n",
    "            name_file='du_country_dataset',\n",
    "            df_spark=country_master,\n",
    "            figure_size_height=2.5\n",
    "        )\n",
    "\n",
    "    def _du_02_meat_consumption(self):\n",
    "        meat_consumption = read_csv(DataFramesCSV.MEAT_CONSUMPTION_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Meat Consumption Dataset',\n",
    "            name_file='du_meat_consumption_dataset',\n",
    "            df_spark=meat_consumption,\n",
    "            figure_size_height=2\n",
    "        )\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=meat_consumption,\n",
    "            dataset_name='Meat Consumption',\n",
    "            name_file='du_meat_consumption_summary',\n",
    "            figure_size_height=2\n",
    "        )\n",
    "\n",
    "        meat_consumption = rename_columns(\n",
    "            df=meat_consumption,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.MEAT_CONSUMPTION_CSV)\n",
    "        )\n",
    "        self._merge_by_country_code(dataset_name='meat_consumption', target_dataset=meat_consumption)\n",
    "\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Beef consumption',\n",
    "            metric_label='beef',\n",
    "            dataset_name='meat_beef',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Poultry consumption',\n",
    "            metric_label='poultry',\n",
    "            dataset_name='meat_poultry',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Sheep and Goat consumption',\n",
    "            metric_label='sheep_and_goat',\n",
    "            dataset_name='meat_sheep',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption.fillna(0),\n",
    "            metric_name_plot='Kg./Year per Capita - Pig consumption',\n",
    "            metric_label='pig',\n",
    "            dataset_name='meat_pig',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=meat_consumption,\n",
    "            metric_name_plot='Kg./Year per Capita - Fish and Seafood consumption',\n",
    "            metric_label='fish_and_seafood',\n",
    "            dataset_name='meat_fish_seafood',\n",
    "            country_label='country_code',\n",
    "        )\n",
    "\n",
    "    def _du_02_hunger(self):\n",
    "        hunger = read_csv(DataFramesCSV.HUNGER_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Hunger Dataset',\n",
    "            name_file='du_hunger_dataset',\n",
    "            df_spark=hunger,\n",
    "            figure_size_height=1.5\n",
    "        )\n",
    "        \n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=hunger,\n",
    "            dataset_name='Hunger',\n",
    "            name_file='du_hunger_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        hunger = hunger.select(\"Entity\", \"Year\", \"Global Hunger Index (2021)\")\n",
    "        hunger = rename_columns(\n",
    "            df=hunger,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.HUNGER_CSV)\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=hunger,\n",
    "            metric_name_plot='Global Hunger Index',\n",
    "            metric_label='hunger_index',\n",
    "            dataset_name='hunger',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='hunger', target_dataset=hunger)\n",
    "\n",
    "    def _du_02_smoking(self):\n",
    "        smoking = read_csv(DataFramesCSV.SMOKING_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Smoking Dataset',\n",
    "            name_file='du_smoking_dataset',\n",
    "            df_spark=smoking,\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        \n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=smoking,\n",
    "            dataset_name='Smoking',\n",
    "            name_file='du_smoking_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        smoking = smoking.drop(\"Code\")        \n",
    "        smoking = rename_columns(\n",
    "            df=smoking,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.SMOKING_CSV)\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=smoking,\n",
    "            metric_name_plot='Percentage Prevalence Tobacco use Adults',\n",
    "            metric_label='prevalence_smoking',\n",
    "            dataset_name='smoking',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='smoking', target_dataset=smoking)\n",
    "\n",
    "    def _du_02_alcohol_consumption(self):\n",
    "        alcohol_consumption = read_csv(DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Alcohol Consumption Dataset',\n",
    "            name_file='du_alcohol_consumption_dataset',\n",
    "            df_spark=alcohol_consumption,\n",
    "            figure_size_height=1.2\n",
    "        )\n",
    "        \n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=alcohol_consumption,\n",
    "            dataset_name='Alcohol Consumption',\n",
    "            name_file='du_alcohol_summary',\n",
    "            figure_size_height=1\n",
    "        )\n",
    "        alcohol_consumption = alcohol_consumption.drop(\"Code\")        \n",
    "        alcohol_consumption = rename_columns(\n",
    "            df=alcohol_consumption,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=alcohol_consumption,\n",
    "            metric_name_plot='Liters of Pure Alcohol per Capita',\n",
    "            metric_label='liters_of_pure_alcohol_per_capita',\n",
    "            dataset_name='alcohol',\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='alcohol_consumption', target_dataset=alcohol_consumption)\n",
    "    \n",
    "    def _du_02_obesity(self):\n",
    "        obesity_dataset = read_csv(DataFramesCSV.WHO_OBESITY_CSV)\n",
    "\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=obesity_dataset,\n",
    "            dataset_name='obesity',\n",
    "            name_file='du_obesity_summary'\n",
    "        )\n",
    "\n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Obesity Dataset',\n",
    "            name_file='du_obesity_dataset',\n",
    "            df_spark=obesity_dataset,\n",
    "            figure_size_height=3.3\n",
    "        )\n",
    "\n",
    "        # Filter the DataFrame based on the 'Sex' column\n",
    "        obesity_dataset = obesity_dataset.filter(obesity_dataset.Sex == \"Both sexes\")\n",
    "\n",
    "        # Select specific columns\n",
    "        obesity_dataset = obesity_dataset.select(\"Numeric\", \"Countries, territories and areas\", \"WHO region\", \"Year\")\n",
    "        \n",
    "        obesity_dataset= rename_columns(\n",
    "            df=obesity_dataset,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.WHO_OBESITY_CSV)\n",
    "        )\n",
    "        self._merge_by_country_name(dataset_name='obesity', target_dataset=obesity_dataset)\n",
    "\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=obesity_dataset.fillna(0),\n",
    "            metric_name_plot='Percentage Obesity',\n",
    "            metric_label='percentage_obesity',\n",
    "            dataset_name='obesity',\n",
    "        )\n",
    "\n",
    "\n",
    "    def _du_02_happiness(self):\n",
    "        happiness_record = read_csv(DataFramesCSV.HAPPINESS_REPORT_CSV)\n",
    "        \n",
    "        self._capture_get_dataframe_info_image(\n",
    "            table_name='Happiness Report Dataset',\n",
    "            name_file='du_happiness_dataset',\n",
    "            df_spark=happiness_record,\n",
    "            figure_size_height=2.5\n",
    "        )\n",
    "\n",
    "        self._capture_summary_dataset_to_image(\n",
    "            df_spark=happiness_record,\n",
    "            dataset_name='Happiness',\n",
    "            name_file='du_happiness_summary',\n",
    "            figure_size_height=2.3\n",
    "        )\n",
    "        map_columns_happiness = {col: col.split(',', 2)[-1].strip() for col in happiness_record.columns}\n",
    "        \n",
    "        happiness_record = rename_columns(\n",
    "            df=happiness_record,\n",
    "            map_columns=map_columns_happiness\n",
    "        )\n",
    "        \n",
    "        happiness_record = rename_columns(\n",
    "            df=happiness_record,\n",
    "            map_columns=COLUMN_RENAME_BY_DATASET.get(DataFramesCSV.HAPPINESS_REPORT_CSV)\n",
    "        )\n",
    "        \n",
    "        self._merge_by_country_name(dataset_name='happiness', target_dataset=happiness_record)\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record,\n",
    "            metric_name_plot='Life Ladder',\n",
    "            metric_label='life_ladder',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Social Support',\n",
    "            metric_label='social_support',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Freedom to Make Life Choices',\n",
    "            metric_label='freedom_to_make_life_choices',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Generosity',\n",
    "            metric_label='generosity',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Perceptions of Corruption',\n",
    "            metric_label='perceptions_of_corruption',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Positive Affect',\n",
    "            metric_label='positive_affect',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "        self._du_data_exploration_basics(\n",
    "            df_spark=happiness_record.fillna(0),\n",
    "            metric_name_plot='Negative affect',\n",
    "            metric_label='negative_affect',\n",
    "            dataset_name='happiness',\n",
    "            country_label='country',\n",
    "        )\n",
    "\n",
    "    def _function_mapper_du_02(self, dataset: DataFramesCSV):\n",
    "        mapper = {\n",
    "            DataFramesCSV.MEAT_CONSUMPTION_CSV: self._du_02_meat_consumption,\n",
    "            DataFramesCSV.WHO_OBESITY_CSV: self._du_02_obesity,\n",
    "            DataFramesCSV.HAPPINESS_REPORT_CSV: self._du_02_happiness,\n",
    "            DataFramesCSV.HUNGER_CSV: self._du_02_hunger,\n",
    "            DataFramesCSV.SMOKING_CSV: self._du_02_smoking,\n",
    "            DataFramesCSV.ALCOHOL_CONSUMPTION_CSV: self._du_02_alcohol_consumption,\n",
    "        }\n",
    "\n",
    "        return mapper.get(dataset)\n",
    "    \n",
    "    def get_crosstab_missing_countries(self, without_replacement=False, save_table=False, head=20) -> pd.DataFrame:\n",
    "        if without_replacement:\n",
    "            filtered_dataframe = self.missing_countries.filter(self.missing_countries[\"replacement\"].isNull())\n",
    "        else:\n",
    "            filtered_dataframe = self.missing_countries\n",
    "        \n",
    "        # Compute the crosstab\n",
    "        crosstab_result = filtered_dataframe.crosstab(\"missing\", \"dataset\")\n",
    "        \n",
    "        # Optionally save the table\n",
    "        if save_table:\n",
    "            if head:\n",
    "                file_name = f'du_missing_countries_per_dataset_head_{head}'\n",
    "            else:\n",
    "                file_name = 'du_missing_countries_per_dataset'\n",
    "            self._capture_table_dataframe_image(\n",
    "                table_name='Missing countries by datasets (20 firsts)',\n",
    "                df_spark=crosstab_result,\n",
    "                col_widths=[0.35, 0.2, 0.1, 0.1, 0.2, 0.1, 0.1, 0.07],\n",
    "                name_file=file_name,\n",
    "                head=head,\n",
    "                font_size=5,\n",
    "                figure_size_height=4.5,\n",
    "            )\n",
    "        \n",
    "        return crosstab_result\n",
    "    \n",
    "    def _du_02_run_processes(self):\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.MEAT_CONSUMPTION_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.WHO_OBESITY_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.HAPPINESS_REPORT_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.HUNGER_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.SMOKING_CSV)()\n",
    "        self._function_mapper_du_02(dataset=DataFramesCSV.ALCOHOL_CONSUMPTION_CSV)()\n",
    "\n",
    "    def du_02(self):\n",
    "        # pd.set_option('display.max_columns', None)\n",
    "        # pd.set_option('display.max_rows', None)\n",
    "        # pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "        self._du_02_country_master()\n",
    "        self._du_02_run_processes()\n",
    "\n",
    "        self.get_crosstab_missing_countries(save_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = CRISPManager(\n",
    "    generate_images_du_02=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "The following method primarily focuses on generating visuals based on various data analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_name_plot  Negative affect\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2199 entries, 0 to 2198\n",
      "Data columns (total 11 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   country                           2199 non-null   object \n",
      " 1   year                              2199 non-null   int32  \n",
      " 2   life_ladder                       2199 non-null   float64\n",
      " 3   Log GDP per capita                2199 non-null   float64\n",
      " 4   social_support                    2199 non-null   float64\n",
      " 5   Healthy life expectancy at birth  2199 non-null   float64\n",
      " 6   freedom_to_make_life_choices      2199 non-null   float64\n",
      " 7   generosity                        2199 non-null   float64\n",
      " 8   perceptions_of_corruption         2199 non-null   float64\n",
      " 9   positive_affect                   2199 non-null   float64\n",
      " 10  negative_affect                   2199 non-null   float64\n",
      "dtypes: float64(9), int32(1), object(1)\n",
      "memory usage: 180.5+ KB\n"
     ]
    }
   ],
   "source": [
    "manager.du_02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
